{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5JNKgC8TfPn"
      },
      "source": [
        "# ECTI-P vs ΛCDM  \n",
        "## Pantheon+SH0ES · BAO · RSD · KiDS · Planck 2018 (distance priors)  \n",
        "### Full MCMC comparison with identical datasets and priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQsYPZuRTkI7"
      },
      "source": [
        "This notebook presents a controlled, like-for-like comparison between the\n",
        "standard flat ΛCDM cosmological model and the ECTI-P model.\n",
        "\n",
        "Both models are tested using:\n",
        "- identical datasets,\n",
        "- identical likelihood definitions,\n",
        "- identical priors where applicable,\n",
        "- identical MCMC configurations.\n",
        "\n",
        "The goal is not model tuning, but a fair statistical comparison based on\n",
        "χ², AIC, and BIC criteria.\n",
        "\n",
        "All chains are automatically saved to disk to prevent data loss in case\n",
        "of disconnection."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Scientific context and scope\n",
        "\n",
        "This notebook presents a **fair, parameter-matched comparison** between:\n",
        "- the standard cosmological model **ΛCDM**\n",
        "- the phenomenological late-time extension **ECTI-P**\n",
        "\n",
        "## Scope (referee-proof)\n",
        "\n",
        "- **ECTI-P is a late-time, background-only modification**: it modifies **only the background expansion** \\(H(z)\\) at low redshift.\n",
        "- **Early-time physics is not modified**: no recombination changes, no Boltzmann solver (CLASS/CAMB), no TT/TE/EE likelihood, no CLik.\n",
        "  Standard radiation content (photons + massless neutrinos) is included where required to compute early-time quantities entering \\((R,\\ \\ell_A,\\ \\omega_b)\\).\n",
        "- **Growth is not modified by ECTI**: the RSD prediction \\(f\\sigma_8\\) is computed using the **standard linear growth equation in GR**, driven by the model background \\(H(z)\\), with **no additional growth parameters**.\n",
        "\n",
        "## Parameterization (FAIR k = 5)\n",
        "\n",
        "Both models are sampled with the same parameter vector:\n",
        "\\[\n",
        "\\theta_5 = (H_0,\\ \\Omega_{m0},\\ \\sigma_8,\\ M,\\ \\omega_b).\n",
        "\\]\n",
        "\n",
        "ECTI-P has two internal parameters \\((\\beta,\\ z_t)\\) that are **fixed constants** (defined in Section 4).  \n",
        "Therefore the comparison is strictly **same-k**: \\(k=5\\) vs \\(k=5\\), and \\(\\Delta \\mathrm{AIC} = \\Delta \\mathrm{BIC} = \\Delta \\chi^2\\).\n",
        "\n",
        "## Datasets used\n",
        "\n",
        "- Pantheon+SH0ES SNe Ia (with full covariance)\n",
        "- DESI 2024 BAO likelihood (mean + covariance)\n",
        "- RSD \\(f\\sigma_8\\) compilation\n",
        "- **KiDS compressed constraint** (Gaussian \\(S_8\\)-like information; **not** the full 2pt weak-lensing likelihood)\n",
        "- Planck 2018 compressed distance priors \\((R,\\ \\ell_A,\\ \\omega_b)\\) with 3×3 covariance\n",
        "- Planck 2018 lensing-only **derived 1D constraint** (optional toggle; counted as 1 datum when enabled)\n"
      ],
      "metadata": {
        "id": "yXoFOCmdSE0E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-G7KKPUc-Y"
      },
      "source": [
        "## 1.1) Environment & execution policy (referee-proof)\n",
        "\n",
        "This section defines:\n",
        "- environment detection and versions\n",
        "- global execution flags\n",
        "- path conventions (server vs Colab)\n",
        "- output folders for chains/figures/tables\n",
        "\n",
        "Policy:\n",
        "- no implicit installs unless explicitly enabled\n",
        "- notebook intended to run top-to-bottom without manual intervention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g6WtqtIL4kj"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1.1.1) Environment setup (JupyterLab / Hetzner / bare metal)\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "def require(pkg, pip_name=None):\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "        print(f\"✔ {pkg} available\")\n",
        "    except ImportError as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Missing dependency '{pkg}'.\\n\"\n",
        "            f\"Install it ONCE on the server with:\\n\"\n",
        "            f\"  pip install {pip_name or pkg}\\n\"\n",
        "            f\"Then restart the Jupyter kernel.\"\n",
        "        ) from e\n",
        "\n",
        "# --- Scientific stack ---\n",
        "require(\"numpy\")\n",
        "require(\"scipy\")\n",
        "require(\"pandas\")\n",
        "require(\"matplotlib\")\n",
        "require(\"emcee\")\n",
        "require(\"corner\")\n",
        "\n",
        "print(\"✔ Environment OK (JupyterLab / server mode)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBxFcn5o-UFs"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1.1.2) Global configuration + PATHS (AUTO: Colab vs JupyterLab)\n",
        "#    + DESI BAO repo clone (optional if already present)\n",
        "#    REFEREE-PROOF:\n",
        "#      - Prefer repo-relative paths (data/rsd/rsd.csv) for portability\n",
        "#      - Keep safe fallbacks for legacy layouts\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import emcee\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Detect environment\n",
        "# -----------------------------\n",
        "IS_COLAB = (\"COLAB_GPU\" in os.environ) or (\"/content\" in os.getcwd()) or (\"google.colab\" in sys.modules)\n",
        "print(f\"✔ Environment detected: {'COLAB' if IS_COLAB else 'JUPYTER / SERVER'}\")\n",
        "print(f\"cwd = {os.getcwd()}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Constants\n",
        "# -----------------------------\n",
        "c_light = 299792.458  # km/s\n",
        "\n",
        "# -----------------------------\n",
        "# KiDS toggle + constants (so Cell #2 never crashes)\n",
        "# -----------------------------\n",
        "USE_KIDS   = True\n",
        "KIDS_MEAN  = 0.766\n",
        "KIDS_SIGMA = 0.020\n",
        "\n",
        "# -----------------------------\n",
        "# MCMC defaults (short-run safe)\n",
        "# -----------------------------\n",
        "N_WALKERS = 16\n",
        "N_BURN    = 2000\n",
        "N_STEPS   = 10000\n",
        "RUN_MCMC  = False\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: pick first existing path\n",
        "# -----------------------------\n",
        "def _first_existing(paths):\n",
        "    for p in paths:\n",
        "        try:\n",
        "            if p and os.path.exists(p):\n",
        "                return p\n",
        "        except Exception:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "CWD = os.getcwd()\n",
        "\n",
        "# -----------------------------\n",
        "# Paths: prefer repo-relative layout\n",
        "#   - Canonical repo layout:\n",
        "#       data/rsd/rsd.csv\n",
        "#       (Pantheon files may be auto-downloaded to CWD or /content)\n",
        "# -----------------------------\n",
        "if IS_COLAB:\n",
        "    # Pantheon goes to /content by default in Colab\n",
        "    PATH_SN_DAT = \"/content/Pantheon+SH0ES.dat\"\n",
        "    PATH_SN_COV = \"/content/Pantheon+SH0ES_STAT+SYS.cov\"\n",
        "else:\n",
        "    # Jupyter/server: default to current project directory\n",
        "    PATH_SN_DAT = os.path.join(CWD, \"Pantheon+SH0ES.dat\")\n",
        "    PATH_SN_COV = os.path.join(CWD, \"Pantheon+SH0ES_STAT+SYS.cov\")\n",
        "\n",
        "# RSD: canonical path in repo + safe fallbacks\n",
        "RSD_CANDIDATES = [\n",
        "    os.path.join(CWD, \"data\", \"rsd\", \"rsd.csv\"),  # canonical GitHub layout\n",
        "    os.path.join(CWD, \"rsd.csv\"),                 # legacy fallback (old layout)\n",
        "    \"/content/data/rsd/rsd.csv\",                   # possible Colab clone layout\n",
        "    \"/content/rsd.csv\",                            # legacy Colab fallback\n",
        "]\n",
        "\n",
        "# If on Colab, also try /content/<repo>/data/rsd/rsd.csv\n",
        "if IS_COLAB and os.path.isdir(\"/content\"):\n",
        "    try:\n",
        "        for name in os.listdir(\"/content\"):\n",
        "            RSD_CANDIDATES.append(os.path.join(\"/content\", name, \"data\", \"rsd\", \"rsd.csv\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "PATH_RSD = _first_existing(RSD_CANDIDATES)\n",
        "\n",
        "# -----------------------------\n",
        "# Pantheon+SH0ES (official DataRelease) — auto-download if missing\n",
        "# SAFE: triggers only if the local files are absent\n",
        "# -----------------------------\n",
        "PANTHEON_DAT_URL = \"https://raw.githubusercontent.com/PantheonPlusSH0ES/DataRelease/main/Pantheon%2B_Data/4_DISTANCES_AND_COVAR/Pantheon%2BSH0ES.dat\"\n",
        "PANTHEON_COV_URL = \"https://raw.githubusercontent.com/PantheonPlusSH0ES/DataRelease/main/Pantheon%2B_Data/4_DISTANCES_AND_COVAR/Pantheon%2BSH0ES_STAT%2BSYS.cov\"\n",
        "\n",
        "def _download_if_missing(url, dest_path, label):\n",
        "    if os.path.isfile(dest_path) and (os.path.getsize(dest_path) > 0):\n",
        "        return\n",
        "    os.makedirs(os.path.dirname(dest_path) or \".\", exist_ok=True)\n",
        "    print(f\"▶ Downloading {label} → {dest_path}\")\n",
        "    try:\n",
        "        import urllib.request\n",
        "        urllib.request.urlretrieve(url, dest_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed to download {label}.\\n\"\n",
        "            f\"URL : {url}\\n\"\n",
        "            f\"DEST: {dest_path}\\n\"\n",
        "            f\"Error: {e}\"\n",
        "        )\n",
        "\n",
        "_download_if_missing(PANTHEON_DAT_URL, PATH_SN_DAT, \"Pantheon+SH0ES.dat\")\n",
        "_download_if_missing(PANTHEON_COV_URL, PATH_SN_COV, \"Pantheon+SH0ES_STAT+SYS.cov\")\n",
        "\n",
        "# -----------------------------\n",
        "# DESI BAO data repo (CobayaSampler/bao_data)\n",
        "# On Colab we can clone into /content; on server into ./bao_data\n",
        "# -----------------------------\n",
        "BAO_REPO_DIR = \"/content/bao_data\" if IS_COLAB else os.path.join(CWD, \"bao_data\")\n",
        "\n",
        "if not os.path.isdir(BAO_REPO_DIR):\n",
        "    print(f\"▶ Cloning https://github.com/CobayaSampler/bao_data.git into {BAO_REPO_DIR} ...\")\n",
        "    rc = os.system(f\"git clone https://github.com/CobayaSampler/bao_data.git {BAO_REPO_DIR}\")\n",
        "    if rc != 0:\n",
        "        raise RuntimeError(\"git clone failed (is git installed / internet available?)\")\n",
        "else:\n",
        "    print(f\"✔ bao_data repo already present: {BAO_REPO_DIR}\")\n",
        "\n",
        "DESI_MEAN_PATH = os.path.join(BAO_REPO_DIR, \"desi_2024_gaussian_bao_ALL_GCcomb_mean.txt\")\n",
        "DESI_COV_PATH  = os.path.join(BAO_REPO_DIR, \"desi_2024_gaussian_bao_ALL_GCcomb_cov.txt\")\n",
        "\n",
        "# -----------------------------\n",
        "# Hard check: required files\n",
        "# -----------------------------\n",
        "if PATH_RSD is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"RSD file not found. Expected one of:\\n\"\n",
        "        \"- ./data/rsd/rsd.csv (recommended, GitHub layout)\\n\"\n",
        "        \"- ./rsd.csv (legacy)\\n\"\n",
        "        \"- /content/<repo>/data/rsd/rsd.csv (Colab clone)\\n\"\n",
        "        \"- /content/rsd.csv (legacy Colab)\\n\"\n",
        "    )\n",
        "\n",
        "required_paths = {\n",
        "    \"PATH_SN_DAT\": PATH_SN_DAT,\n",
        "    \"PATH_SN_COV\": PATH_SN_COV,\n",
        "    \"PATH_RSD\": PATH_RSD,\n",
        "    \"DESI_MEAN\": DESI_MEAN_PATH,\n",
        "    \"DESI_COV\": DESI_COV_PATH,\n",
        "}\n",
        "\n",
        "missing = [k for k, v in required_paths.items() if not os.path.exists(v)]\n",
        "\n",
        "print(\"\\n--- Required paths ---\")\n",
        "for k, v in required_paths.items():\n",
        "    print(f\"{k} -> {v} | exists={os.path.exists(v)}\")\n",
        "\n",
        "if missing:\n",
        "    raise FileNotFoundError(\n",
        "        \"Missing required file(s):\\n- \" + \"\\n- \".join([f\"{k}: {required_paths[k]}\" for k in missing]) +\n",
        "        \"\\n\\nFix:\\n\"\n",
        "        \"- On COLAB: ensure the repo is cloned (so ./data/rsd/rsd.csv exists) or upload rsd.csv to /content.\\n\"\n",
        "        \"- On Jupyter/server: run from the repo root (so ./data/rsd/rsd.csv exists), or place files accordingly.\\n\"\n",
        "    )\n",
        "\n",
        "print(\"\\n✔ Required files found (SN, RSD, DESI BAO mean+cov)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1.1.3) LOCAL OUTPUT PATHS (JupyterLab) + RUN NUMBERING + RESUME\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Base output folder (LOCAL, no Drive) ---\n",
        "BASE_RUN_DIR = os.path.abspath(\"ECTI_runs\")\n",
        "CHAINS_DIR   = os.path.join(BASE_RUN_DIR, \"chains_sigma8free\")\n",
        "FIG_DIR      = os.path.join(CHAINS_DIR, \"figures\")\n",
        "TAB_DIR      = os.path.join(CHAINS_DIR, \"tables\")\n",
        "META_DIR     = os.path.join(CHAINS_DIR, \"_meta\")\n",
        "\n",
        "for d in [BASE_RUN_DIR, CHAINS_DIR, FIG_DIR, TAB_DIR, META_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# --- Behaviour flags ---\n",
        "RESUME_IF_INCOMPLETE = True     # reprend le dernier run si backend.iteration < N_STEPS\n",
        "WRITE_LATEST_COPY    = True     # copie pratique vers *_latest.h5 après chaque run\n",
        "\n",
        "def _list_runs(model_tag: str):\n",
        "    \"\"\"\n",
        "    Retourne la liste triée des fichiers: CHAINS_DIR/{model_tag}_runXXX.h5\n",
        "    \"\"\"\n",
        "    pat = re.compile(rf\"^{re.escape(model_tag)}_run(\\d+)\\.h5$\")\n",
        "    found = []\n",
        "    for fn in os.listdir(CHAINS_DIR):\n",
        "        m = pat.match(fn)\n",
        "        if m:\n",
        "            found.append((int(m.group(1)), os.path.join(CHAINS_DIR, fn)))\n",
        "    found.sort(key=lambda x: x[0])\n",
        "    return [p for _, p in found]\n",
        "\n",
        "def latest_run_path(model_tag: str):\n",
        "    runs = _list_runs(model_tag)\n",
        "    return runs[-1] if runs else None\n",
        "\n",
        "def next_run_path(model_tag: str):\n",
        "    runs = _list_runs(model_tag)\n",
        "    if not runs:\n",
        "        return os.path.join(CHAINS_DIR, f\"{model_tag}_run001.h5\")\n",
        "    last = runs[-1]\n",
        "    m = re.search(r\"run(\\d+)\\.h5$\", last)\n",
        "    last_idx = int(m.group(1)) if m else 0\n",
        "    return os.path.join(CHAINS_DIR, f\"{model_tag}_run{last_idx+1:03d}.h5\")\n",
        "\n",
        "def copy_as_latest(src_h5: str, model_tag: str):\n",
        "    \"\"\"\n",
        "    Copie le fichier run vers {model_tag}_latest.h5 (pratique pour lecture rapide).\n",
        "    \"\"\"\n",
        "    if not WRITE_LATEST_COPY:\n",
        "        return None\n",
        "    dst = os.path.join(CHAINS_DIR, f\"{model_tag}_latest.h5\")\n",
        "    shutil.copy2(src_h5, dst)\n",
        "    return dst\n",
        "\n",
        "print(\"✔ Jupyter local outputs ready\")\n",
        "print(\"  BASE_RUN_DIR:\", BASE_RUN_DIR)\n",
        "print(\"  CHAINS_DIR  :\", CHAINS_DIR)\n",
        "print(\"  FIG_DIR     :\", FIG_DIR)\n",
        "print(\"  TAB_DIR     :\", TAB_DIR)"
      ],
      "metadata": {
        "id": "dMtl645nN5KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBGzYWyOX6VV",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 2) Observational datasets and validation\n",
        "This section loads all observational datasets used in the analysis\n",
        "(Pantheon+ SN Ia, BAO, RSD, KiDS, Planck compressed priors, and Planck lensing),\n",
        "and performs strict shape and consistency checks before any likelihood evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PEiW-22BsWj"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2.1) Data loading and validation (SN, DESI BAO, RSD, KiDS, CMB)\n",
        "#   - DESI cov loaded with np.loadtxt (NO regex) to avoid misparsing\n",
        "#   - Handles shapes: (12,12), (24,12), (12,24)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Safety: ensure DESI_MEAN_PATH / DESI_COV_PATH are defined\n",
        "# (prevents NameError if Cell #1.1.2 was modified or not executed)\n",
        "# ------------------------------------------------------------\n",
        "if (\"DESI_MEAN_PATH\" not in globals()) or (\"DESI_COV_PATH\" not in globals()):\n",
        "    BAO_REPO_DIR = globals().get(\"BAO_REPO_DIR\", os.path.join(os.getcwd(), \"bao_data\"))\n",
        "    DESI_MEAN_PATH = os.path.join(BAO_REPO_DIR, \"desi_2024_gaussian_bao_ALL_GCcomb_mean.txt\")\n",
        "    DESI_COV_PATH  = os.path.join(BAO_REPO_DIR, \"desi_2024_gaussian_bao_ALL_GCcomb_cov.txt\")\n",
        "\n",
        "if not os.path.exists(DESI_MEAN_PATH):\n",
        "    raise FileNotFoundError(f\"DESI mean file not found: {DESI_MEAN_PATH}\")\n",
        "if not os.path.exists(DESI_COV_PATH):\n",
        "    raise FileNotFoundError(f\"DESI cov file not found: {DESI_COV_PATH}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Supernovae: Pantheon+SH0ES\n",
        "# ------------------------------------------------------------\n",
        "sn = np.genfromtxt(PATH_SN_DAT, dtype=None, names=True, encoding=\"utf-8\")\n",
        "\n",
        "required_cols = [\"zHD\", \"m_b_corr\"]\n",
        "for col in required_cols:\n",
        "    if col not in sn.dtype.names:\n",
        "        raise ValueError(\n",
        "            f\"SN file missing required column '{col}'. Available columns: {sn.dtype.names}\"\n",
        "        )\n",
        "\n",
        "z_SN  = sn[\"zHD\"].astype(float)\n",
        "mu_SN = sn[\"m_b_corr\"].astype(float)\n",
        "N_SN  = len(z_SN)\n",
        "\n",
        "print(\"=== Supernovae (Pantheon+SH0ES) ===\")\n",
        "print(f\"N_SN     = {N_SN}\")\n",
        "print(f\"z range   = [{z_SN.min():.6f}, {z_SN.max():.6f}]\")\n",
        "print(\"observable= m_b_corr (stored in mu_SN for legacy naming)\")\n",
        "print(\"\")\n",
        "\n",
        "cov_flat = np.loadtxt(PATH_SN_COV)\n",
        "N_cov = int(cov_flat[0])\n",
        "C_SN = cov_flat[1:].reshape(N_cov, N_cov)\n",
        "\n",
        "if N_cov != N_SN:\n",
        "    raise ValueError(f\"SN covariance mismatch: N_cov={N_cov}, N_SN={N_SN}\")\n",
        "\n",
        "Cinv_SN = np.linalg.inv(C_SN)\n",
        "\n",
        "print(\"=== SN covariance ===\")\n",
        "print(f\"C_SN shape   = {C_SN.shape}\")\n",
        "print(f\"Cinv_SN shape= {Cinv_SN.shape}\")\n",
        "print(\"\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# DESI DR1 BAO mean + covariance (bao_data)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "def _load_mean_file_robust(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            if (not s) or s.startswith(\"#\"):\n",
        "                continue\n",
        "            nums = re.findall(r\"[-+]?\\d*\\.\\d+|[-+]?\\d+\", s)\n",
        "            if len(nums) >= 2:\n",
        "                rows.append([float(nums[0]), float(nums[1])])\n",
        "    if len(rows) == 0:\n",
        "        raise RuntimeError(f\"Mean file unreadable/empty: {path}\")\n",
        "    return np.array(rows, dtype=float)\n",
        "\n",
        "mean_arr = _load_mean_file_robust(DESI_MEAN_PATH)  # (N,2): z, obs\n",
        "z_BAO   = mean_arr[:, 0].astype(float)\n",
        "BAO_obs = mean_arr[:, 1].astype(float)\n",
        "N_BAO_VEC = BAO_obs.size\n",
        "\n",
        "# --- Load cov with np.loadtxt (reliable)\n",
        "raw_cov = np.loadtxt(DESI_COV_PATH, dtype=float)\n",
        "\n",
        "print(\"=== DESI DR1 BAO (mean + cov) ===\")\n",
        "print(f\"Vector dim = {N_BAO_VEC}\")\n",
        "print(f\"raw_cov shape from np.loadtxt = {raw_cov.shape}\")\n",
        "\n",
        "# --- Normalize to a 12x12 covariance\n",
        "if raw_cov.shape == (N_BAO_VEC, N_BAO_VEC):\n",
        "    BAO_cov = raw_cov\n",
        "\n",
        "elif raw_cov.shape == (2*N_BAO_VEC, N_BAO_VEC):\n",
        "    # two blocks stacked vertically: try top or bottom\n",
        "    C_top = raw_cov[:N_BAO_VEC, :]\n",
        "    C_bot = raw_cov[N_BAO_VEC:, :]\n",
        "\n",
        "    # choose the one with positive diagonal and more PSD-ish\n",
        "    def _score(C):\n",
        "        C = 0.5*(C + C.T)\n",
        "        dmin = float(np.min(np.diag(C)))\n",
        "        eigmin = float(np.min(np.linalg.eigvalsh(C)))\n",
        "        return (dmin, eigmin)\n",
        "\n",
        "    s_top = _score(C_top)\n",
        "    s_bot = _score(C_bot)\n",
        "    BAO_cov = C_top if (s_top[0] > s_bot[0]) else C_bot\n",
        "    print(f\"cov stacked (24x12): chose {'TOP' if BAO_cov is C_top else 'BOTTOM'} block | scores top={s_top}, bot={s_bot}\")\n",
        "\n",
        "elif raw_cov.shape == (N_BAO_VEC, 2*N_BAO_VEC):\n",
        "    # two blocks side-by-side: try left or right\n",
        "    C_left = raw_cov[:, :N_BAO_VEC]\n",
        "    C_right = raw_cov[:, N_BAO_VEC:]\n",
        "\n",
        "    def _score(C):\n",
        "        C = 0.5*(C + C.T)\n",
        "        dmin = float(np.min(np.diag(C)))\n",
        "        eigmin = float(np.min(np.linalg.eigvalsh(C)))\n",
        "        return (dmin, eigmin)\n",
        "\n",
        "    s_left = _score(C_left)\n",
        "    s_right = _score(C_right)\n",
        "    BAO_cov = C_left if (s_left[0] > s_right[0]) else C_right\n",
        "    print(f\"cov wide (12x24): chose {'LEFT' if BAO_cov is C_left else 'RIGHT'} block | scores left={s_left}, right={s_right}\")\n",
        "\n",
        "else:\n",
        "    raise RuntimeError(\n",
        "        f\"Unexpected DESI cov shape {raw_cov.shape}. \"\n",
        "        f\"Expected (12,12), (24,12) or (12,24) for dim={N_BAO_VEC}.\"\n",
        "    )\n",
        "\n",
        "# Symmetrize + strict diagnostics\n",
        "BAO_cov = 0.5 * (BAO_cov + BAO_cov.T)\n",
        "diag = np.diag(BAO_cov)\n",
        "eig = np.linalg.eigvalsh(BAO_cov)\n",
        "\n",
        "print(\"--- DESI_BAO_cov diagnostics ---\")\n",
        "print(f\"min diag     = {float(np.min(diag)):.6e}\")\n",
        "print(f\"median diag  = {float(np.median(diag)):.6e}\")\n",
        "print(f\"min eig      = {float(np.min(eig)):.6e}\")\n",
        "print(f\"max eig      = {float(np.max(eig)):.6e}\")\n",
        "\n",
        "if float(np.min(diag)) <= 0:\n",
        "    raise RuntimeError(\"DESI BAO covariance has non-positive diagonal after selection. STOP.\")\n",
        "if float(np.min(eig)) < -1e-10:\n",
        "    raise RuntimeError(\"DESI BAO covariance is not PSD (min eigenvalue too negative). STOP.\")\n",
        "\n",
        "BAO_inv_cov = np.linalg.inv(BAO_cov)\n",
        "print(\"✔ DESI BAO covariance selected + inverted successfully\")\n",
        "print(\"\")\n",
        "\n",
        "# Infer row-kind by repetition in z (DM then DH; singleton -> DV)\n",
        "def _row_kind(z_eff):\n",
        "    z = np.asarray(z_eff, dtype=float)\n",
        "    n = len(z)\n",
        "    kind = []\n",
        "    for i in range(n):\n",
        "        prev_same = (i > 0 and np.isclose(z[i], z[i-1], rtol=0, atol=1e-12))\n",
        "        next_same = (i < n-1 and np.isclose(z[i], z[i+1], rtol=0, atol=1e-12))\n",
        "        if next_same and not prev_same:\n",
        "            kind.append(\"DM_over_rd\")\n",
        "        elif prev_same and not next_same:\n",
        "            kind.append(\"DH_over_rd\")\n",
        "        else:\n",
        "            kind.append(\"DV_over_rd\")\n",
        "    return kind\n",
        "\n",
        "BAO_row_kind = _row_kind(z_BAO)\n",
        "\n",
        "print(pd.Series(BAO_row_kind).value_counts().to_string())\n",
        "print(\"First 12 rows (z, kind, obs):\")\n",
        "for i in range(min(12, N_BAO_VEC)):\n",
        "    print(f\"  i={i:02d}  z={z_BAO[i]:.6f}  {BAO_row_kind[i]:10s}  obs={BAO_obs[i]:.6f}\")\n",
        "print(\"✔ DESI BAO loaded + invcov ready\")\n",
        "print(\"\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# RSD (your existing loader)\n",
        "# ------------------------------------------------------------\n",
        "rsd_df = pd.read_csv(PATH_RSD)\n",
        "\n",
        "required_cols = [\"z\", \"fs8\", \"err\"]\n",
        "for col in required_cols:\n",
        "    if col not in rsd_df.columns:\n",
        "        raise ValueError(\n",
        "            f\"RSD file missing required column '{col}'. Columns found: {list(rsd_df.columns)}\"\n",
        "        )\n",
        "\n",
        "z_RSD   = rsd_df[\"z\"].values.astype(float)\n",
        "fs8_obs = rsd_df[\"fs8\"].values.astype(float)\n",
        "fs8_err = rsd_df[\"err\"].values.astype(float)\n",
        "\n",
        "N_RSD = len(z_RSD)\n",
        "\n",
        "print(\"=== RSD data ===\")\n",
        "print(f\"N_RSD   = {N_RSD}\")\n",
        "print(f\"z range = [{z_RSD.min():.6f}, {z_RSD.max():.6f}]\")\n",
        "print(\"\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# KiDS (Gaussian prior on S8)\n",
        "# ------------------------------------------------------------\n",
        "if USE_KIDS:\n",
        "    print(\"=== KiDS prior ===\")\n",
        "    print(f\"S8 = {KIDS_MEAN} ± {KIDS_SIGMA}\")\n",
        "    print(\"\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# CMB compressed priors (already loaded elsewhere in your notebook)\n",
        "# Here: only the shape/availability check to avoid silent None\n",
        "# ------------------------------------------------------------\n",
        "if \"PLANCK_R_MEAN\" in globals() and \"PLANCK_R_SIGMA\" in globals():\n",
        "    print(\"=== CMB Planck compressed ===\")\n",
        "    print(f\"R     = {PLANCK_R_MEAN} ± {PLANCK_R_SIGMA}\")\n",
        "    if \"PLANCK_lA_MEAN\" in globals() and \"PLANCK_lA_SIGMA\" in globals():\n",
        "        print(f\"lA    = {PLANCK_lA_MEAN} ± {PLANCK_lA_SIGMA}\")\n",
        "    print(\"\")\n",
        "else:\n",
        "    print(\"=== CMB Planck compressed ===\")\n",
        "    print(\"⚠ PLANCK_R_MEAN / PLANCK_R_SIGMA not found in globals (check your Planck cell order).\")\n",
        "    print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“If this section runs without error, all datasets are loaded, validated, and ready for likelihood evaluation.”"
      ],
      "metadata": {
        "id": "4GPVwTT7aFBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) ΛCDM background model (H(z), distances, SN μ(z), BAO DV/rs)\n",
        "\n",
        "This section defines the **flat ΛCDM background cosmology** used as our baseline model.\n",
        "\n",
        "We implement:\n",
        "- the dimensionless expansion rate \\(E(z)=H(z)/H_0\\),\n",
        "- the Hubble rate \\(H(z)\\),\n",
        "- comoving distance \\(D_C(z)\\),\n",
        "- luminosity distance \\(D_L(z)\\),\n",
        "- supernova distance modulus \\(\\mu(z)\\),\n",
        "- BAO volume distance \\(D_V(z)\\) and the observable \\(D_V(z)/r_s\\).\n",
        "\n",
        "These functions are written in a **single, consolidated block** and are used consistently\n",
        "throughout the notebook for both:\n",
        "- likelihood evaluation (χ²),\n",
        "- plotting and diagnostics.\n",
        "\n",
        "> Important: this block must remain unique in the notebook (no redefinitions later),\n",
        "to guarantee reproducibility and prevent hidden inconsistencies."
      ],
      "metadata": {
        "id": "EGj25PjJaWNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3.1) ΛCDM background model (H(z), distances, SN μ(z), BAO DV/rs)\n",
        "#    ROBUST: scalar + array + defines ALL functions used later\n",
        "#    UPDATED: BAO uses computed r_s(z*) via wb (baryon density)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import quad\n",
        "\n",
        "_trap = getattr(np, \"trapezoid\", np.trapz)\n",
        "c_light = 299792.458\n",
        "\n",
        "def _wb_default():\n",
        "    # fallback referee-proof: if wb not provided in theta, we take WB_FIXED if it exists\n",
        "    if \"WB_FIXED\" in globals():\n",
        "        try:\n",
        "            return float(globals()[\"WB_FIXED\"])\n",
        "        except Exception:\n",
        "            pass\n",
        "    # safe fallback\n",
        "    return 0.02237\n",
        "\n",
        "def _unpack_lcdm_theta(theta):\n",
        "    \"\"\"\n",
        "    Robust unpack for LCDM vectors used across the notebook.\n",
        "\n",
        "    Accepts common signatures:\n",
        "      - (H0, Om0, M)\n",
        "      - (H0, Om0, s8, M)\n",
        "      - (H0, Om0, M, wb)\n",
        "      - (H0, Om0, s8, M, wb)\n",
        "\n",
        "    Returns: (H0, Om0, s8_or_nan, M, wb)\n",
        "    \"\"\"\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size == 3:\n",
        "        H0, Om0, M = th\n",
        "        return float(H0), float(Om0), np.nan, float(M), float(_wb_default())\n",
        "\n",
        "    if th.size == 4:\n",
        "        # Ambiguous: either (H0,Om0,s8,M) OR (H0,Om0,M,wb)\n",
        "        H0, Om0, a, b = th\n",
        "\n",
        "        # Heuristic by physical ranges:\n",
        "        # M ~ [-25,-15], wb ~ [0.005,0.04], sigma8 ~ [0.3,1.3]\n",
        "        looks_like_M_wb = (-25.0 < a < -15.0) and (0.005 < b < 0.04)\n",
        "        looks_like_s8_M = (0.3 < a < 1.3) and (-25.0 < b < -15.0)\n",
        "\n",
        "        if looks_like_M_wb and (not looks_like_s8_M):\n",
        "            M, wb = a, b\n",
        "            return float(H0), float(Om0), np.nan, float(M), float(wb)\n",
        "\n",
        "        # default to (H0,Om0,s8,M)\n",
        "        s8, M = a, b\n",
        "        return float(H0), float(Om0), float(s8), float(M), float(_wb_default())\n",
        "\n",
        "    if th.size == 5:\n",
        "        H0, Om0, s8, M, wb = th\n",
        "        return float(H0), float(Om0), float(s8), float(M), float(wb)\n",
        "\n",
        "    raise ValueError(f\"LCDM theta must have size 3, 4 or 5; got {th.size}\")\n",
        "\n",
        "def get_rs_star(H0, Om0, wb):\n",
        "    \"\"\"\n",
        "    Horizon sonore approx. au découplage (Aubourg et al. 2015 style).\n",
        "    wb = Omega_b * h^2.\n",
        "    \"\"\"\n",
        "    h = float(H0) / 100.0\n",
        "    om = float(Om0) * h**2\n",
        "    wb = float(wb)\n",
        "    return 147.06 * ((om / 0.1417)**-0.25) * ((wb / 0.02219)**-0.12)\n",
        "\n",
        "def E_LCDM(z, H0, Om0):\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return np.sqrt(Om0 * (1.0 + z) ** 3 + (1.0 - Om0))\n",
        "\n",
        "def H_LCDM(z, H0, Om0):\n",
        "    return float(H0) * E_LCDM(z, H0, Om0)\n",
        "\n",
        "def D_C_LCDM(z, H0, Om0, n=4000):\n",
        "    z_arr = np.atleast_1d(np.asarray(z, dtype=float))\n",
        "    DC = np.zeros_like(z_arr, dtype=float)\n",
        "    for i, zz in enumerate(z_arr):\n",
        "        if zz <= 0.0:\n",
        "            DC[i] = 0.0\n",
        "            continue\n",
        "        z_grid = np.linspace(0.0, float(zz), int(n))\n",
        "        Ez = E_LCDM(z_grid, H0, Om0)\n",
        "        DC[i] = (c_light / float(H0)) * _trap(1.0 / Ez, x=z_grid)\n",
        "    return DC[0] if np.isscalar(z) else DC\n",
        "\n",
        "def D_M_LCDM(z, H0, Om0, n=4000):\n",
        "    return D_C_LCDM(z, H0, Om0, n=n)\n",
        "\n",
        "def D_L_LCDM(z, H0, Om0, n=4000):\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return (1.0 + z) * D_M_LCDM(z, H0, Om0, n=n)\n",
        "\n",
        "def mu_LCDM(z, theta, n=4000):\n",
        "    \"\"\"\n",
        "    SN distance modulus.\n",
        "    Accepts theta sizes 3/4/5 (see _unpack_lcdm_theta).\n",
        "    \"\"\"\n",
        "    H0, Om0, _s8, M, _wb = _unpack_lcdm_theta(theta)\n",
        "    DL = D_L_LCDM(z, H0, Om0, n=n)\n",
        "    return 5.0 * np.log10(DL) + 25.0 + M\n",
        "\n",
        "def DV_LCDM(z, theta, n=4000):\n",
        "    \"\"\"\n",
        "    BAO volume distance:\n",
        "    D_V(z) = [ (c z D_M(z)^2) / H(z) ]^(1/3)\n",
        "    Robust to theta sizes 3/4/5 (only uses H0,Om0).\n",
        "    \"\"\"\n",
        "    H0, Om0, _s8, _M, _wb = _unpack_lcdm_theta(theta)\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    DM = D_M_LCDM(z, H0, Om0, n=n)\n",
        "    Hz = H_LCDM(z, H0, Om0)\n",
        "    return ((c_light * z * DM**2) / Hz) ** (1.0/3.0)\n",
        "\n",
        "def DV_over_rs_LCDM(z, theta, n=4000):\n",
        "    \"\"\"\n",
        "    D_V(z) / r_s with r_s depending on wb (last parameter if provided).\n",
        "    \"\"\"\n",
        "    H0, Om0, _s8, _M, wb = _unpack_lcdm_theta(theta)\n",
        "    dv = DV_LCDM(z, theta, n=n)\n",
        "    rs = get_rs_star(H0, Om0, wb)\n",
        "    return dv / rs\n",
        "\n",
        "print(\"✔ ΛCDM background ready: E,H, D_C/D_M/D_L, mu, DV, DV/rs (wb parsed safely)\")"
      ],
      "metadata": {
        "id": "yKeVVtZMacsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3.2) EARLY-TIME PHYSICS (referee-proof)\n",
        "#   - Radiation included (photons + massless neutrinos, Neff=3.046)\n",
        "#   - Sound horizons:\n",
        "#       r_s(z*) at decoupling (z* fixed = 1089.92 by notebook design)\n",
        "#       r_d at drag epoch via Eisenstein & Hu z_d fit + numeric integral\n",
        "#   - Single source of truth for CMB/BAO coherence with wb\n",
        "#\n",
        "# Units:\n",
        "#   H0 in km/s/Mpc, distances in Mpc, c in km/s.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from numpy import trapezoid as _trap\n",
        "\n",
        "# Physical constants / standards\n",
        "c_light = float(globals().get(\"c_light\", 299792.458))  # km/s\n",
        "T_CMB = 2.7255\n",
        "N_EFF = 3.046\n",
        "\n",
        "Z_STAR = float(globals().get(\"Z_STAR\", 1089.92))  # decoupling (fixed by design)\n",
        "\n",
        "def _omega_gamma_h2(Tcmb=T_CMB):\n",
        "    # Photon density today: ωγ = Ωγ h^2\n",
        "    return 2.469e-5 * (Tcmb / 2.7255)**4\n",
        "\n",
        "def _omega_nu_h2(omega_g_h2, Neff=N_EFF):\n",
        "    # Massless neutrinos: ων = ωγ * (7/8)*(4/11)^(4/3)*Neff\n",
        "    return omega_g_h2 * (7.0/8.0) * (4.0/11.0)**(4.0/3.0) * Neff\n",
        "\n",
        "def _E_early(z, H0, Om0, wb):\n",
        "    \"\"\"\n",
        "    Early-time E(z) including radiation.\n",
        "    Flat Universe: ΩΛ = 1 - Ωm - Ωr\n",
        "    Inputs:\n",
        "      wb = ωb = Ωb h^2\n",
        "    \"\"\"\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    h = float(H0) / 100.0\n",
        "\n",
        "    omega_g = _omega_gamma_h2()\n",
        "    omega_nu = _omega_nu_h2(omega_g)\n",
        "\n",
        "    Or0 = (omega_g + omega_nu) / (h*h)\n",
        "    Om0 = float(Om0)\n",
        "    Ol0 = 1.0 - Om0 - Or0\n",
        "\n",
        "    E2 = Or0*(1.0+z)**4 + Om0*(1.0+z)**3 + Ol0\n",
        "    if np.any(E2 <= 0.0):\n",
        "        return np.full_like(z, np.nan, dtype=float)\n",
        "    return np.sqrt(E2)\n",
        "\n",
        "def _R_b_over_gamma(z, H0, wb):\n",
        "    \"\"\"\n",
        "    R(z) = 3ρb/(4ργ) = (3Ωb)/(4Ωγ) * 1/(1+z)\n",
        "    with Ωb = wb/h^2, Ωγ = ωγ/h^2\n",
        "    \"\"\"\n",
        "    h = float(H0) / 100.0\n",
        "    omega_g = _omega_gamma_h2()\n",
        "    Ob0 = float(wb) / (h*h)\n",
        "    Og0 = omega_g / (h*h)\n",
        "    return (3.0*Ob0)/(4.0*Og0) * 1.0/(1.0+np.asarray(z, float))\n",
        "\n",
        "def D_M_early(z, H0, Om0, wb, n=12000):\n",
        "    \"\"\"\n",
        "    Comoving transverse distance using early-time E(z) (includes radiation).\n",
        "    \"\"\"\n",
        "    z = float(z)\n",
        "    if z <= 0.0:\n",
        "        return 0.0\n",
        "    zz = np.linspace(0.0, z, int(n), dtype=float)\n",
        "    Ez = _E_early(zz, H0, Om0, wb)\n",
        "    return (c_light / float(H0)) * float(_trap(1.0/Ez, x=zz))\n",
        "\n",
        "def _rs_integral(z_upper, H0, Om0, wb, z_max=1.0e7, n=20000):\n",
        "    \"\"\"\n",
        "    r_s(z_upper) = (c/H0) ∫_{z_upper}^{z_max} [ c_s(z)/c ] dz / E(z)\n",
        "    where c_s/c = 1/sqrt(3(1+R(z)))\n",
        "    \"\"\"\n",
        "    z_upper = float(z_upper)\n",
        "    if z_upper <= 0.0:\n",
        "        raise ValueError(\"z_upper must be > 0 for sound horizon integral.\")\n",
        "    z_max = float(z_max)\n",
        "\n",
        "    # Log grid in (1+z) for stability\n",
        "    u1 = np.log(1.0 + z_upper)\n",
        "    u2 = np.log(1.0 + z_max)\n",
        "    uu = np.linspace(u1, u2, int(n), dtype=float)\n",
        "    zz = np.exp(uu) - 1.0\n",
        "\n",
        "    Ez = _E_early(zz, H0, Om0, wb)\n",
        "    Rz = _R_b_over_gamma(zz, H0, wb)\n",
        "    cs_over_c = 1.0 / np.sqrt(3.0 * (1.0 + Rz))\n",
        "\n",
        "    integrand = cs_over_c / Ez\n",
        "    # dz = (1+z) du\n",
        "    rs = (c_light / float(H0)) * float(_trap(integrand * (1.0 + zz), x=uu))\n",
        "    return rs\n",
        "\n",
        "def z_drag_EH(H0, Om0, wb):\n",
        "    \"\"\"\n",
        "    Eisenstein & Hu (1998) fitting formula for drag redshift z_d.\n",
        "    Uses ωm=Ωm h^2, ωb=wb.\n",
        "    \"\"\"\n",
        "    h = float(H0)/100.0\n",
        "    wm = float(Om0) * h*h\n",
        "    wb = float(wb)\n",
        "\n",
        "    b1 = 0.313 * (wm**(-0.419)) * (1.0 + 0.607*(wm**0.674))\n",
        "    b2 = 0.238 * (wm**0.223)\n",
        "    zd = 1291.0 * (wm**0.251) / (1.0 + 0.659*(wm**0.828)) * (1.0 + b1*(wb**b2))\n",
        "    return float(zd)\n",
        "\n",
        "def rs_star(H0, Om0, wb):\n",
        "    return _rs_integral(Z_STAR, H0, Om0, wb)\n",
        "\n",
        "def rd_drag(H0, Om0, wb):\n",
        "    zd = z_drag_EH(H0, Om0, wb)\n",
        "    return _rs_integral(zd, H0, Om0, wb)\n",
        "\n",
        "print(\"✔ Early-time module ready: E_early(z), D_M_early, r_s(z*), r_d(z_d) with radiation + wb\")"
      ],
      "metadata": {
        "id": "esG5w7B1alZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) ECTI-P background model (H(z), distances, SN μ(z), BAO DV/rs)\n",
        "\n",
        "This section defines the **ECTI-P** background expansion model used in this notebook.\n",
        "\n",
        "ECTI-P is implemented as a **low-redshift modification** of the ΛCDM expansion rate:\n",
        "\\[\n",
        "H^2_{\\rm ECTI}(a) = H^2_{\\Lambda{\\rm CDM}}(a)\\,\\left[1 + \\beta\\,S(a; z_t)\\right],\n",
        "\\]\n",
        "where:\n",
        "- \\(\\beta\\) controls the amplitude of the late-time correction,\n",
        "- \\(S(a; z_t)\\) is a smooth activation (\"switch\") function centered around \\(z_t\\),\n",
        "- the model is designed to behave as ΛCDM at high redshift (early universe).\n",
        "\n",
        "We implement:\n",
        "- the switch function \\(S(a)\\),\n",
        "- \\(H(z)\\) and \\(E(z)=H/H_0\\),\n",
        "- comoving and luminosity distances \\(D_C(z)\\), \\(D_L(z)\\),\n",
        "- SN distance modulus \\(\\mu(z)\\),\n",
        "- BAO volume distance \\(D_V(z)\\) and \\(D_V(z)/r_s\\).\n",
        "\n",
        "This block is **the unique ECTI-P background definition** and must not be redefined later."
      ],
      "metadata": {
        "id": "rktu71WIau0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4.1) ECTI background model (FAIR k=5 vs ΛCDM, β & zt fixed)\n",
        "#    θ5 = (H0, Om0, σ8, M, wb)\n",
        "#    Backward-compatible unpack supports θ6/θ7 (legacy), but MCMC uses θ5.\n",
        "#    r_s(z*) / early physics: SAME as ΛCDM (uses get_rs_star with wb).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# --- Fixed ECTI parameters (used when theta has no beta/zt) ---\n",
        "BETA_FIXED = float(globals().get(\"BETA_FIXED\", -0.10))\n",
        "ZT_FIXED   = float(globals().get(\"ZT_FIXED\",   0.10))\n",
        "\n",
        "_trap = getattr(np, \"trapezoid\", np.trapz)\n",
        "\n",
        "def _unpack_ecti_theta(theta):\n",
        "    \"\"\"\n",
        "    Robust ECTI unpacker (referee-proof).\n",
        "    Supported:\n",
        "      θ5 = (H0, Om0, σ8, M, wb)                 [FAIR comparison mode]\n",
        "      θ6 = (H0, Om0, σ8, beta, zt, M)           [legacy]\n",
        "      θ7 = (H0, Om0, σ8, beta, zt, M, wb)       [legacy]\n",
        "    Returns: (H0, Om0, σ8, beta, zt, M, wb)\n",
        "    \"\"\"\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size == 5:\n",
        "        H0, Om0, s8, M, wb = th\n",
        "        return float(H0), float(Om0), float(s8), BETA_FIXED, ZT_FIXED, float(M), float(wb)\n",
        "\n",
        "    if th.size == 6:\n",
        "        H0, Om0, s8, beta, zt, M = th\n",
        "        wb = float(globals().get(\"WB_FIXED\", 0.02237))\n",
        "        return float(H0), float(Om0), float(s8), float(beta), float(zt), float(M), float(wb)\n",
        "\n",
        "    if th.size == 7:\n",
        "        H0, Om0, s8, beta, zt, M, wb = th\n",
        "        return float(H0), float(Om0), float(s8), float(beta), float(zt), float(M), float(wb)\n",
        "\n",
        "    raise ValueError(f\"ECTI theta must have size 5/6/7; got {th.size}\")\n",
        "\n",
        "# Backward-compat alias used by some older cells\n",
        "def _unpack_ecti(theta):\n",
        "    return _unpack_ecti_theta(theta)\n",
        "\n",
        "def E_ECTI(z, theta):\n",
        "    \"\"\"\n",
        "    E(z) for ECTI-P (late-time-only modification, smooth, no extra hyper-params):\n",
        "      E^2(z) = Om0(1+z)^3 + (1-Om0) * exp[ beta * exp(-z/zt) ]\n",
        "    Properties:\n",
        "      - As z >> zt: exp(-z/zt) -> 0 => DE term -> exp(0)=1 (ΛCDM recovered)\n",
        "      - At z=0: DE term -> exp(beta)\n",
        "    \"\"\"\n",
        "    H0, Om0, _s8, beta, zt, _M, _wb = _unpack_ecti_theta(theta)\n",
        "    z = np.asarray(z, dtype=float)\n",
        "\n",
        "    if zt <= 0.0:\n",
        "        raise ValueError(\"ECTI requires zt > 0 (transition scale).\")\n",
        "\n",
        "    de_factor = np.exp(beta * np.exp(-z / zt))\n",
        "    E2 = Om0 * (1.0 + z)**3 + (1.0 - Om0) * de_factor\n",
        "\n",
        "    if np.any(E2 <= 0.0):\n",
        "        return np.full_like(z, np.nan, dtype=float)\n",
        "\n",
        "    return np.sqrt(E2)\n",
        "\n",
        "def H_ECTI(z, theta):\n",
        "    H0, *_ = _unpack_ecti_theta(theta)\n",
        "    return H0 * E_ECTI(z, theta)\n",
        "\n",
        "def D_C_ECTI(z, theta, n=4000):\n",
        "    H0, *_ = _unpack_ecti_theta(theta)\n",
        "    z_arr = np.atleast_1d(np.asarray(z, dtype=float))\n",
        "    DC = np.zeros_like(z_arr, dtype=float)\n",
        "\n",
        "    for i, zi in enumerate(z_arr):\n",
        "        if zi <= 0.0:\n",
        "            DC[i] = 0.0\n",
        "            continue\n",
        "        zz = np.linspace(0.0, zi, int(n), dtype=float)\n",
        "        Ez = E_ECTI(zz, theta)\n",
        "        DC[i] = (c_light / H0) * float(_trap(1.0 / Ez, x=zz))\n",
        "\n",
        "    return DC if np.ndim(z) else float(DC[0])\n",
        "\n",
        "def D_M_ECTI(z, theta, n=4000):\n",
        "    return D_C_ECTI(z, theta, n=n)\n",
        "\n",
        "def D_L_ECTI(z, theta, n=4000):\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    return (1.0 + z) * D_M_ECTI(z, theta, n=n)\n",
        "\n",
        "def mu_ECTI(z, theta, n=4000):\n",
        "    H0, Om0, _s8, _beta, _zt, M, _wb = _unpack_ecti_theta(theta)\n",
        "    DL = D_L_ECTI(z, theta, n=n)\n",
        "    return 5.0 * np.log10(DL) + 25.0 + M\n",
        "\n",
        "def DH_ECTI(z, theta):\n",
        "    return c_light / H_ECTI(z, theta)\n",
        "\n",
        "def DV_ECTI(z, theta, n=4000):\n",
        "    z = np.asarray(z, dtype=float)\n",
        "    DM = D_M_ECTI(z, theta, n=n)\n",
        "    DH = DH_ECTI(z, theta)\n",
        "    return (DM**2 * (z * DH))**(1.0/3.0)\n",
        "\n",
        "def _rs_from_theta(theta):\n",
        "    H0, Om0, _s8, _beta, _zt, _M, wb = _unpack_ecti_theta(theta)\n",
        "    return get_rs_star(H0, Om0, wb)\n",
        "\n",
        "def DM_over_rs_ECTI(z, theta, n=4000):\n",
        "    return D_M_ECTI(z, theta, n=n) / _rs_from_theta(theta)\n",
        "\n",
        "def DH_over_rs_ECTI(z, theta):\n",
        "    return DH_ECTI(z, theta) / _rs_from_theta(theta)\n",
        "\n",
        "def DV_over_rs_ECTI(z, theta, n=4000):\n",
        "    return DV_ECTI(z, theta, n=n) / _rs_from_theta(theta)\n",
        "\n",
        "print(\"✔ ECTI background ready (FAIR k=5): E,H, distances, mu, BAO ratios, wb-compatible\")"
      ],
      "metadata": {
        "id": "_jPLPnCwa3Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By construction, the ECTI-P background reduces exactly to ΛCDM when β = 0, independently of the choice of zₜ; this identity is explicitly verified in the proof cell below."
      ],
      "metadata": {
        "id": "tALakPVBjwpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PROOF CELL — ECTI reduces to ΛCDM when beta = 0 (zt arbitrary > 0)\n",
        "#   Referee-proof identity checks:\n",
        "#     (1) E(z) equality on grid\n",
        "#     (2) Distance equality: D_M, D_L\n",
        "#     (3) mu equality on z>0 (avoid log10(DL=0) at z=0)\n",
        "#     (4) BAO: DV/rs equality\n",
        "#   Output explicitly states that zt is arbitrary and irrelevant when beta=0.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_required = [\n",
        "    \"E_LCDM\", \"D_M_LCDM\", \"D_L_LCDM\", \"mu_LCDM\", \"DV_over_rs_LCDM\",\n",
        "    \"E_ECTI\", \"D_M_ECTI\", \"D_L_ECTI\", \"mu_ECTI\", \"DV_over_rs_ECTI\",\n",
        "]\n",
        "_missing = [k for k in _required if k not in globals()]\n",
        "if _missing:\n",
        "    raise NameError(\"Missing required symbols before PROOF cell: \" + \", \".join(_missing))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Generic θ5 test point (must be within priors)\n",
        "# θ5 = (H0, Om0, σ8, M, wb)\n",
        "# ------------------------------------------------------------\n",
        "theta5 = np.array([67.0, 0.315, 0.80, -19.40, 0.02237], dtype=float)\n",
        "H0, Om0, s8, M, wb = theta5\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Construct ECTI theta with beta=0 and arbitrary zt>0.\n",
        "# Your ECTI unpacker supports θ7=(H0,Om0,s8,beta,zt,M,wb).\n",
        "# Key identity: beta=0 => exp[beta*exp(-z/zt)] = 1 for all z, hence ΛCDM exactly.\n",
        "# zt is only required to be >0 to satisfy the model guard; it is physically irrelevant when beta=0.\n",
        "# ------------------------------------------------------------\n",
        "beta0 = 0.0\n",
        "zt_any = float(globals().get(\"ZT_FIXED\", 0.10))\n",
        "if zt_any <= 0.0:\n",
        "    zt_any = 0.10  # purely to satisfy zt>0 guard; value irrelevant when beta=0\n",
        "\n",
        "theta7_beta0 = np.array([H0, Om0, s8, beta0, zt_any, M, wb], dtype=float)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Numerical tolerances (referee-proof)\n",
        "# ------------------------------------------------------------\n",
        "rtol_E   = 3e-10\n",
        "rtol_D   = 5e-9\n",
        "atol_mu  = 5e-10   # absolute tolerance for mu equality (more stable than relative)\n",
        "rtol_bao = 2e-8\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# z grid (include 0 for E and D; exclude 0 for mu to avoid log10(DL=0))\n",
        "# ------------------------------------------------------------\n",
        "z_grid = np.concatenate([np.linspace(0.0, 0.30, 120), np.linspace(0.30, 3.0, 180)])\n",
        "z_mu   = z_grid[z_grid > 0.0]\n",
        "\n",
        "# 1) E(z)\n",
        "E_lcdm = np.asarray(E_LCDM(z_grid, H0, Om0), float)\n",
        "E_ecti = np.asarray(E_ECTI(z_grid, theta7_beta0), float)\n",
        "relE = float(np.max(np.abs(E_ecti - E_lcdm) / np.maximum(1e-15, np.abs(E_lcdm))))\n",
        "\n",
        "# 2) distances\n",
        "DM_l = np.asarray(D_M_LCDM(z_grid, H0, Om0), float)\n",
        "DM_e = np.asarray(D_M_ECTI(z_grid, theta7_beta0), float)\n",
        "relDM = float(np.max(np.abs(DM_e - DM_l) / np.maximum(1e-12, np.abs(DM_l))))\n",
        "\n",
        "DL_l = np.asarray(D_L_LCDM(z_grid, H0, Om0), float)\n",
        "DL_e = np.asarray(D_L_ECTI(z_grid, theta7_beta0), float)\n",
        "relDL = float(np.max(np.abs(DL_e - DL_l) / np.maximum(1e-12, np.abs(DL_l))))\n",
        "\n",
        "# 3) mu(z) on z>0 only\n",
        "mu_l = np.asarray(mu_LCDM(z_mu, theta5), float)\n",
        "mu_e = np.asarray(mu_ECTI(z_mu, theta7_beta0), float)\n",
        "max_abs_dmu = float(np.max(np.abs(mu_e - mu_l)))\n",
        "\n",
        "# 4) BAO DV/rs\n",
        "if \"BAO_z\" in globals():\n",
        "    z_bao = np.asarray(globals()[\"BAO_z\"], float).ravel()\n",
        "else:\n",
        "    z_bao = np.array([0.15, 0.32, 0.51, 0.70, 1.48], dtype=float)\n",
        "\n",
        "bao_l = np.asarray(DV_over_rs_LCDM(z_bao, theta5), float).ravel()\n",
        "bao_e = np.asarray(DV_over_rs_ECTI(z_bao, theta7_beta0), float).ravel()\n",
        "relbao = float(np.max(np.abs(bao_e - bao_l) / np.maximum(1e-12, np.abs(bao_l))))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Assertions (fail hard if identity breaks)\n",
        "# ------------------------------------------------------------\n",
        "if not np.allclose(E_ecti, E_lcdm, rtol=rtol_E, atol=0.0):\n",
        "    raise RuntimeError(f\"E(z) identity failed at beta=0. max_rel={relE:.3e}\")\n",
        "if not np.allclose(DM_e, DM_l, rtol=rtol_D, atol=0.0):\n",
        "    raise RuntimeError(f\"D_M identity failed at beta=0. max_rel={relDM:.3e}\")\n",
        "if not np.allclose(DL_e, DL_l, rtol=rtol_D, atol=0.0):\n",
        "    raise RuntimeError(f\"D_L identity failed at beta=0. max_rel={relDL:.3e}\")\n",
        "if not np.allclose(mu_e, mu_l, rtol=0.0, atol=atol_mu):\n",
        "    raise RuntimeError(f\"mu identity failed at beta=0. max_abs_dmu={max_abs_dmu:.3e}\")\n",
        "if not np.allclose(bao_e, bao_l, rtol=rtol_bao, atol=0.0):\n",
        "    raise RuntimeError(f\"DV/rs identity failed at beta=0. max_rel={relbao:.3e}\")\n",
        "\n",
        "print(\"====================================================\")\n",
        "print(\"PROOF: ECTI → ΛCDM when β = 0 (zt arbitrary > 0)\")\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"theta5 (ΛCDM)       =\", theta5, \"  (H0,Om0,σ8,M,wb)\")\n",
        "print(\n",
        "    \"theta7 (ECTI, β=0)  =\", theta7_beta0,\n",
        "    f\"\\n  with zt={zt_any:.3f} chosen arbitrarily (>0) — physically irrelevant when β=0\"\n",
        ")\n",
        "print(\"----------------------------------------------------\")\n",
        "print(f\"max_rel |E|     = {relE:.3e}   (rtol={rtol_E})\")\n",
        "print(f\"max_rel |D_M|   = {relDM:.3e}  (rtol={rtol_D})\")\n",
        "print(f\"max_rel |D_L|   = {relDL:.3e}  (rtol={rtol_D})\")\n",
        "print(f\"max_abs |Δmu|   = {max_abs_dmu:.3e} (atol={atol_mu})  [z>0]\")\n",
        "print(f\"max_rel |DV/rs| = {relbao:.3e} (rtol={rtol_bao})\")\n",
        "print(\"====================================================\")\n",
        "print(\"✔ Identity checks PASSED (numerical equality within tolerance).\")"
      ],
      "metadata": {
        "id": "4K6OKxzebANM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Likelihood blocks (χ² per probe)\n",
        "\n",
        "This section defines the independent χ² contributions for each probe:\n",
        "SN, BAO, RSD, KiDS, Planck compressed prior (R, ℓA, ωb), and Planck lensing-derived 1D constraint.\n"
      ],
      "metadata": {
        "id": "PlE_rG1qbOul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.1) Supernovae likelihood (Pantheon+SH0ES: m_b_corr + full cov)\n",
        "#    FAIR: θ5 = (H0, Om0, σ8, M, wb) for BOTH ΛCDM and ECTI\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def chi2_SN_LCDM(theta):\n",
        "    mu_th = mu_LCDM(z_SN, theta)\n",
        "    diff = mu_SN - mu_th\n",
        "    return float(diff @ Cinv_SN @ diff)\n",
        "\n",
        "def chi2_SN_ECTI(theta):\n",
        "    mu_th = mu_ECTI(z_SN, theta)\n",
        "    diff = mu_SN - mu_th\n",
        "    return float(diff @ Cinv_SN @ diff)\n",
        "\n",
        "def lnlikelihood_sn_lcdm(theta):\n",
        "    return -0.5 * chi2_SN_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_sn_ecti(theta):\n",
        "    return -0.5 * chi2_SN_ECTI(theta)\n",
        "\n",
        "print(\"✔ SN likelihood ready (ΛCDM/ECTI): chi2_SN_*, lnlikelihood_sn_*\")\n"
      ],
      "metadata": {
        "id": "hyLeaRhbVhhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.2) DESI DR1 BAO likelihood (mean + cov)\n",
        "#    Coherent early-time: uses r_d(wb) from Cell 3bis (radiation + wb)\n",
        "#    FAIR: θ5 = (H0, Om0, σ8, M, wb)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_required = [\"z_BAO\", \"BAO_obs\", \"BAO_inv_cov\", \"BAO_row_kind\",\n",
        "             \"D_M_LCDM\", \"H_LCDM\",\n",
        "             \"D_M_ECTI\", \"DH_ECTI\", \"DV_ECTI\",\n",
        "             \"rd_drag\", \"c_light\"]\n",
        "_missing = [n for n in _required if n not in globals()]\n",
        "if _missing:\n",
        "    raise NameError(\"Missing required symbols before BAO cell: \" + \", \".join(_missing))\n",
        "\n",
        "def _theta5(theta, label=\"theta\"):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(f\"{label} must be θ5=(H0,Om0,σ8,M,wb); got size {th.size}\")\n",
        "    return th\n",
        "\n",
        "def bao_theory_vector_LCDM(theta, n=4000):\n",
        "    th = _theta5(theta, \"BAO LCDM theta\")\n",
        "    H0, Om0, _s8, _M, wb = th\n",
        "    rd = rd_drag(H0, Om0, wb)\n",
        "\n",
        "    vec = np.zeros_like(BAO_obs, dtype=float)\n",
        "    for i, (zi, kind) in enumerate(zip(z_BAO, BAO_row_kind)):\n",
        "        zi = float(zi)\n",
        "        if kind == \"DM_over_rd\":\n",
        "            dm = D_M_LCDM(zi, float(H0), float(Om0), n=n)\n",
        "            vec[i] = dm / rd\n",
        "        elif kind == \"DH_over_rd\":\n",
        "            dh = c_light / H_LCDM(zi, float(H0), float(Om0))\n",
        "            vec[i] = dh / rd\n",
        "        elif kind == \"DV_over_rd\":\n",
        "            # DV = (DM^2 * z * DH)^(1/3)\n",
        "            dm = D_M_LCDM(zi, float(H0), float(Om0), n=n)\n",
        "            dh = c_light / H_LCDM(zi, float(H0), float(Om0))\n",
        "            dv = (dm*dm * (zi*dh))**(1.0/3.0)\n",
        "            vec[i] = dv / rd\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown BAO_row_kind: {kind}\")\n",
        "    return vec\n",
        "\n",
        "def bao_theory_vector_ECTI(theta, n=4000):\n",
        "    th = _theta5(theta, \"BAO ECTI theta\")\n",
        "    H0, Om0, _s8, _M, wb = th\n",
        "    rd = rd_drag(H0, Om0, wb)  # same early-time physics\n",
        "\n",
        "    vec = np.zeros_like(BAO_obs, dtype=float)\n",
        "    for i, (zi, kind) in enumerate(zip(z_BAO, BAO_row_kind)):\n",
        "        zi = float(zi)\n",
        "        if kind == \"DM_over_rd\":\n",
        "            vec[i] = D_M_ECTI(zi, th, n=n) / rd\n",
        "        elif kind == \"DH_over_rd\":\n",
        "            vec[i] = DH_ECTI(zi, th) / rd\n",
        "        elif kind == \"DV_over_rd\":\n",
        "            vec[i] = DV_ECTI(zi, th, n=n) / rd\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown BAO_row_kind: {kind}\")\n",
        "    return vec\n",
        "\n",
        "def chi2_BAO_LCDM(theta):\n",
        "    t = bao_theory_vector_LCDM(theta)\n",
        "    diff = BAO_obs - t\n",
        "    return float(diff @ BAO_inv_cov @ diff)\n",
        "\n",
        "def chi2_BAO_ECTI(theta):\n",
        "    t = bao_theory_vector_ECTI(theta)\n",
        "    diff = BAO_obs - t\n",
        "    return float(diff @ BAO_inv_cov @ diff)\n",
        "\n",
        "def lnlikelihood_bao_lcdm(theta):\n",
        "    return -0.5 * chi2_BAO_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_bao_ecti(theta):\n",
        "    return -0.5 * chi2_BAO_ECTI(theta)\n",
        "\n",
        "print(\"✔ BAO likelihood ready (coherent r_d(wb) + radiation): chi2_BAO_*\")\n"
      ],
      "metadata": {
        "id": "6EsjdI7-VxqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.3) RSD likelihood — fσ₈(z) with linear growth ODE (σ8 FREE)\n",
        "#    FAIR: θ5 = (H0, Om0, σ8, M, wb) for BOTH ΛCDM and ECTI\n",
        "#    Growth uses the SAME background E(z) definitions as SN/BAO.\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "\n",
        "_required = [\"z_RSD\", \"fs8_obs\", \"fs8_err\"]\n",
        "_missing = [n for n in _required if n not in globals()]\n",
        "if _missing:\n",
        "    raise NameError(\"Missing required RSD arrays before RSD cell: \" + \", \".join(_missing))\n",
        "\n",
        "def _check_theta5(theta, label):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(f\"{label} expects θ5=(H0,Om0,σ8,M,wb); got size {th.size}\")\n",
        "    return th\n",
        "\n",
        "def _E_of_z(model, z, theta):\n",
        "    if model == \"LCDM\":\n",
        "        H0, Om0, *_ = _unpack_lcdm_theta(theta)\n",
        "        return E_LCDM(z, H0, Om0)\n",
        "    elif model == \"ECTI\":\n",
        "        return E_ECTI(z, theta)\n",
        "    else:\n",
        "        raise ValueError(\"model must be 'LCDM' or 'ECTI'\")\n",
        "\n",
        "def _dlnH_dlnA(model, a, theta):\n",
        "    # numerical derivative in ln a (robust, no hard-coded formula)\n",
        "    # H(a) = H0 * E(z) with z=1/a-1\n",
        "    eps = 1e-4\n",
        "    lna = np.log(a)\n",
        "    a1 = np.exp(lna - eps)\n",
        "    a2 = np.exp(lna + eps)\n",
        "    z1 = 1.0 / a1 - 1.0\n",
        "    z2 = 1.0 / a2 - 1.0\n",
        "    H1 = _E_of_z(model, z1, theta)\n",
        "    H2 = _E_of_z(model, z2, theta)\n",
        "    # H0 cancels in derivative\n",
        "    return float((np.log(H2) - np.log(H1)) / (2.0 * eps))\n",
        "\n",
        "def _growth_solution(model, theta, a_min=1e-3):\n",
        "    \"\"\"\n",
        "    Solve for D(a) with initial conditions deep in matter era:\n",
        "      D(a_min) = a_min\n",
        "      dD/da(a_min) = 1\n",
        "    Then normalize to D(1)=1.\n",
        "    Returns interpolants via dense_output for D(a) and dD/da(a).\n",
        "    \"\"\"\n",
        "    th = _check_theta5(theta, f\"growth_solution[{model}]\")\n",
        "    H0, Om0, sigma8, _M, _wb = th\n",
        "\n",
        "    def rhs(a, y):\n",
        "        # y = [D, dD/da]\n",
        "        D, dDda = y\n",
        "        z = 1.0 / a - 1.0\n",
        "        E = float(_E_of_z(model, z, th))\n",
        "        if not np.isfinite(E) or E <= 0.0:\n",
        "            return [np.nan, np.nan]\n",
        "\n",
        "        dlnH_dlnA = _dlnH_dlnA(model, a, th)\n",
        "\n",
        "        # Equation: D'' + [ (3/a) + (dlnH/dlna)/a ] D' - (3/2) Om0 /(a^5 E^2) D = 0\n",
        "        # Note: H(a)^2 = H0^2 E^2; H0 cancels.\n",
        "        coeff_fric = (3.0 + dlnH_dlnA) / a\n",
        "        coeff_src  = 1.5 * Om0 / (a**5 * E**2)\n",
        "\n",
        "        d2Dda2 = -coeff_fric * dDda + coeff_src * D\n",
        "        return [dDda, d2Dda2]\n",
        "\n",
        "    y0 = [a_min, 1.0]\n",
        "    sol = solve_ivp(\n",
        "        rhs, t_span=(a_min, 1.0), y0=y0, rtol=1e-6, atol=1e-9,\n",
        "        dense_output=True, max_step=0.02\n",
        "    )\n",
        "    if not sol.success:\n",
        "        raise RuntimeError(f\"Growth ODE failed for {model}: {sol.message}\")\n",
        "\n",
        "    # Normalize to D(1)=1\n",
        "    D1 = float(sol.y[0, -1])\n",
        "    def D_of_a(a):\n",
        "        return sol.sol(a)[0] / D1\n",
        "    def dDda_of_a(a):\n",
        "        return sol.sol(a)[1] / D1\n",
        "\n",
        "    return D_of_a, dDda_of_a\n",
        "\n",
        "def fsigma8_theory(theta, model=\"LCDM\"):\n",
        "    th = _check_theta5(theta, f\"fsigma8_theory[{model}]\")\n",
        "    sigma8 = float(th[2])\n",
        "\n",
        "    D_of_a, dDda_of_a = _growth_solution(model, th)\n",
        "\n",
        "    fs8 = np.zeros_like(z_RSD, dtype=float)\n",
        "    for i, z in enumerate(z_RSD):\n",
        "        a = 1.0 / (1.0 + float(z))\n",
        "        D  = float(D_of_a(a))\n",
        "        dDda = float(dDda_of_a(a))\n",
        "        if D <= 0.0 or not np.isfinite(D) or not np.isfinite(dDda):\n",
        "            fs8[i] = np.nan\n",
        "            continue\n",
        "        f = (a / D) * dDda  # d ln D / d ln a\n",
        "        fs8[i] = f * sigma8 * D\n",
        "    return fs8\n",
        "\n",
        "def chi2_RSD_LCDM(theta):\n",
        "    pred = fsigma8_theory(theta, model=\"LCDM\")\n",
        "    diff = (fs8_obs - pred) / fs8_err\n",
        "    return float(np.dot(diff, diff))\n",
        "\n",
        "def chi2_RSD_ECTI(theta):\n",
        "    pred = fsigma8_theory(theta, model=\"ECTI\")\n",
        "    diff = (fs8_obs - pred) / fs8_err\n",
        "    return float(np.dot(diff, diff))\n",
        "\n",
        "def lnlikelihood_rsd_lcdm(theta):\n",
        "    return -0.5 * chi2_RSD_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_rsd_ecti(theta):\n",
        "    return -0.5 * chi2_RSD_ECTI(theta)\n",
        "\n",
        "print(\"✔ RSD likelihood ready (ΛCDM/ECTI): chi2_RSD_*, lnlikelihood_rsd_*\")"
      ],
      "metadata": {
        "id": "RUAupoyQWWVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 KiDS (compressed weak-lensing constraint)\n",
        "\n",
        "KiDS likelihood.  \n",
        "We use a compressed KiDS constraint (effective S₈-like information), rather than the full weak-lensing two-point correlation function likelihood.  \n",
        "This choice is consistent with the background-only scope of this work and avoids introducing additional perturbation-level modeling.\n"
      ],
      "metadata": {
        "id": "52VzCyFnsUPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.4) KiDS S8 prior likelihood (σ8 FREE)\n",
        "#    FAIR: same S8 definition for ΛCDM and ECTI\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _S8_from_theta(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(\"KiDS expects θ5=(H0,Om0,σ8,M,wb).\")\n",
        "    Om0 = float(th[1])\n",
        "    s8  = float(th[2])\n",
        "    return s8 * np.sqrt(Om0 / 0.3)\n",
        "\n",
        "def chi2_KiDS_LCDM(theta):\n",
        "    if not globals().get(\"USE_KIDS\", False):\n",
        "        return 0.0\n",
        "    S8 = _S8_from_theta(theta)\n",
        "    return float(((S8 - KIDS_MEAN) / KIDS_SIGMA) ** 2)\n",
        "\n",
        "def chi2_KiDS_ECTI(theta):\n",
        "    return chi2_KiDS_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_kids_lcdm(theta):\n",
        "    return -0.5 * chi2_KiDS_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_kids_ecti(theta):\n",
        "    return -0.5 * chi2_KiDS_ECTI(theta)\n",
        "\n",
        "print(\"✔ KiDS prior ready (ΛCDM/ECTI): chi2_KiDS_*, lnlikelihood_kids_*\")\n"
      ],
      "metadata": {
        "id": "F6ptq6TOWiEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.5) CMB COMPRESSED LIKELIHOOD (PLANCK lite: R + ℓA + wb, 3×3)\n",
        "#    Coherent early-time: D_M(z*) + r_s(z*) include radiation + wb\n",
        "#    FAIR: θ5 = (H0, Om0, σ8, M, wb) for BOTH ΛCDM and ECTI\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_required = [\"D_M_early\", \"rs_star\", \"c_light\", \"Z_STAR\"]\n",
        "_missing = [n for n in _required if n not in globals()]\n",
        "if _missing:\n",
        "    raise NameError(\"Missing required symbols before CMB cell: \" + \", \".join(_missing))\n",
        "\n",
        "# Planck lite compressed observations [R, lA, wb]\n",
        "CMB_OBS = np.array([1.7502, 301.471, 0.02237], dtype=float)\n",
        "\n",
        "# Covariance (R, lA, wb)\n",
        "CMB_COV = np.array([\n",
        "    [2.125e-05, -1.650e-04, -1.000e-07],\n",
        "    [-1.650e-04, 8.836e-03,  1.300e-06],\n",
        "    [-1.000e-07, 1.300e-06,  2.250e-08]\n",
        "], dtype=float)\n",
        "CMB_INV_COV = np.linalg.inv(CMB_COV)\n",
        "\n",
        "def _theta5(theta, label=\"theta\"):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(f\"{label} must be θ5=(H0,Om0,σ8,M,wb); got size {th.size}\")\n",
        "    return th\n",
        "\n",
        "def _cmb_vector_R_lA_wb(theta):\n",
        "    th = _theta5(theta, \"CMB theta\")\n",
        "    H0, Om0, _s8, _M, wb = th\n",
        "\n",
        "    dm_star = D_M_early(float(Z_STAR), float(H0), float(Om0), float(wb))\n",
        "    rs      = rs_star(float(H0), float(Om0), float(wb))\n",
        "\n",
        "    lA = np.pi * dm_star / rs\n",
        "    R  = np.sqrt(float(Om0)) * (float(H0) / float(c_light)) * dm_star\n",
        "    return np.array([R, lA, wb], dtype=float)\n",
        "\n",
        "def chi2_CMB_LCDM(theta):\n",
        "    vec = _cmb_vector_R_lA_wb(theta)\n",
        "    diff = vec - CMB_OBS\n",
        "    return float(diff @ CMB_INV_COV @ diff)\n",
        "\n",
        "def chi2_CMB_ECTI(theta):\n",
        "    # FAIR: same early-time constraint, depends only on (H0, Om0, wb)\n",
        "    return chi2_CMB_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_cmb_lcdm(theta):\n",
        "    return -0.5 * chi2_CMB_LCDM(theta)\n",
        "\n",
        "def lnlikelihood_cmb_ecti(theta):\n",
        "    return -0.5 * chi2_CMB_ECTI(theta)\n",
        "\n",
        "print(\"✔ CMB compressed ready (Planck lite 3D): radiation + r_s(z*) + wb coherent\")"
      ],
      "metadata": {
        "id": "q4r0mZqsW1M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 5.6) Planck lensing-derived 1D prior (summary constraint; lensing-only)\n",
        "#   This is NOT the Planck lensing likelihood (no C_L^{phi-phi}, no covariance).\n",
        "#   It is a published 1D derived constraint from Planck 2018 lensing-only analysis:\n",
        "#\n",
        "#     sigma8 * Omega_m^alpha = mean ± sigma   with alpha=0.25\n",
        "#\n",
        "#   Usage policy (referee-proof):\n",
        "#     - chi2_PLANCK_LENSING(theta) ALWAYS returns the χ² value.\n",
        "#     - The toggle USE_PLANCK_LENSING is applied ONLY in chi2_tot / chi2_breakdown.\n",
        "#     - No new parameter (k unchanged).\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Toggle (applied in chi2_tot / chi2_breakdown; NOT inside chi2_PLANCK_LENSING)\n",
        "USE_PLANCK_LENSING = bool(globals().get(\"USE_PLANCK_LENSING\", True))\n",
        "\n",
        "# --- Published lensing-only derived constraint (Planck 2018 lensing) ---\n",
        "LENSING_S8OM025_ALPHA = float(globals().get(\"LENSING_S8OM025_ALPHA\", 0.25))\n",
        "LENSING_S8OM025_MEAN  = float(globals().get(\"LENSING_S8OM025_MEAN\",  0.589))\n",
        "LENSING_S8OM025_SIGMA = float(globals().get(\"LENSING_S8OM025_SIGMA\", 0.020))\n",
        "\n",
        "# Traceability (string for paper/README; keep it in the notebook globals)\n",
        "LENSING_S8OM025_REF = str(globals().get(\n",
        "    \"LENSING_S8OM025_REF\",\n",
        "    \"Planck Collaboration VIII (Planck 2018 lensing-only; A&A 641, A8 (2020); see text/eq. for sigma8*Omega_m^0.25 = 0.589±0.020)\"\n",
        "))\n",
        "\n",
        "def lensing_combo_s8_om025(theta):\n",
        "    \"\"\"\n",
        "    Returns: sigma8 * Om0^alpha from theta5 = (H0, Om0, sigma8, M, wb).\n",
        "    \"\"\"\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size < 5:\n",
        "        raise ValueError(f\"lensing_combo_s8_om025 expects θ5 (>=5); got {th.size}\")\n",
        "\n",
        "    Om0    = float(th[1])\n",
        "    sigma8 = float(th[2])\n",
        "    alpha  = float(LENSING_S8OM025_ALPHA)\n",
        "\n",
        "    # Hard physical guards (avoid silent nonsense)\n",
        "    if (not np.isfinite(Om0)) or (not np.isfinite(sigma8)):\n",
        "        return np.nan\n",
        "    if Om0 <= 0.0 or sigma8 <= 0.0:\n",
        "        return np.nan\n",
        "\n",
        "    return sigma8 * (Om0 ** alpha)\n",
        "\n",
        "def chi2_PLANCK_LENSING(theta):\n",
        "    \"\"\"\n",
        "    Independent 1D χ² block:\n",
        "      χ² = ((combo - mean)/sigma)^2\n",
        "    NOTE: toggle is NOT here (handled by chi2_tot / chi2_breakdown).\n",
        "    \"\"\"\n",
        "    combo = float(lensing_combo_s8_om025(theta))\n",
        "    if not np.isfinite(combo):\n",
        "        return np.inf\n",
        "    d = (combo - float(LENSING_S8OM025_MEAN)) / float(LENSING_S8OM025_SIGMA)\n",
        "    return float(d * d)\n",
        "\n",
        "print(\"✔ Planck lensing-derived 1D prior loaded (summary constraint)\")\n",
        "print(f\"  USE_PLANCK_LENSING = {USE_PLANCK_LENSING}\")\n",
        "print(f\"  combo = sigma8 * Om0^{LENSING_S8OM025_ALPHA:.2f}\")\n",
        "print(f\"  mean ± sigma = {LENSING_S8OM025_MEAN:.3f} ± {LENSING_S8OM025_SIGMA:.3f}\")\n",
        "print(f\"  ref = {LENSING_S8OM025_REF}\")"
      ],
      "metadata": {
        "id": "mNVg9IpVXDjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, all independent χ² contributions entering the global likelihood\n",
        "have been defined. The total likelihood is constructed as the sum of these blocks,\n",
        "ensuring modularity, transparency, and full control over probe inclusion.\n"
      ],
      "metadata": {
        "id": "MuEA_bGhctom"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7TdYO2JcjkB",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 6) Priors, total χ², and log-probability (ΛCDM and ECTI-P)\n",
        "\n",
        "This section defines:\n",
        "- parameter priors (uniform bounds),\n",
        "- the total χ² for each model as the sum of all enabled datasets,\n",
        "- the log-probability function used by `emcee`.\n",
        "\n",
        "The goal is to ensure a **single, explicit, auditable definition** of:\n",
        "- what parameters are sampled,\n",
        "- what priors are applied,\n",
        "- which datasets contribute to the likelihood.\n",
        "\n",
        "From this point onward, ΛCDM and ECTI-P are fully specified and ready for MCMC sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BERZzLUs7dqJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6.1) Priors + total chi2 + lnprob (FAIR k=5, σ8 FREE, wb FREE)\n",
        "#    θ5 = (H0, Om0, σ8, M, wb) for BOTH ΛCDM and ECTI\n",
        "#\n",
        "#    UPDATED:\n",
        "#      - Adds Planck lensing derived 1D χ² block (no new params)\n",
        "#      - Ensures chi2_tot includes LENS iff USE_PLANCK_LENSING=True\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def lnprior_common(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        return -np.inf\n",
        "    H0, Om0, sigma8, M, wb = th\n",
        "\n",
        "    # --- Broad, explicit, auditable bounds (uniform) ---\n",
        "    if not (40.0 < H0 < 95.0):\n",
        "        return -np.inf\n",
        "    if not (0.05 < Om0 < 0.6):\n",
        "        return -np.inf\n",
        "    if not (0.2 < sigma8 < 1.6):\n",
        "        return -np.inf\n",
        "    if not (-25.0 < M < -15.0):\n",
        "        return -np.inf\n",
        "    if not (0.005 < wb < 0.04):\n",
        "        return -np.inf\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "def lnprior_LCDM(theta):\n",
        "    return lnprior_common(theta)\n",
        "\n",
        "def lnprior_ECTI(theta):\n",
        "    return lnprior_common(theta)\n",
        "\n",
        "def _chi2_lensing_if_on(theta):\n",
        "    if not bool(globals().get(\"USE_PLANCK_LENSING\", False)):\n",
        "        return 0.0\n",
        "    if \"chi2_PLANCK_LENSING\" not in globals():\n",
        "        raise NameError(\"USE_PLANCK_LENSING=True but chi2_PLANCK_LENSING is not defined (Cell 5.6 missing).\")\n",
        "    return float(globals()[\"chi2_PLANCK_LENSING\"](theta))\n",
        "\n",
        "def chi2_tot_LCDM(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        return np.inf\n",
        "\n",
        "    chi2 = (\n",
        "        float(chi2_SN_LCDM(th))\n",
        "        + float(chi2_BAO_LCDM(th))\n",
        "        + float(chi2_RSD_LCDM(th))\n",
        "        + float(chi2_CMB_LCDM(th))\n",
        "    )\n",
        "\n",
        "    if bool(globals().get(\"USE_KIDS\", False)):\n",
        "        chi2 += float(chi2_KiDS_LCDM(th))\n",
        "\n",
        "    chi2 += _chi2_lensing_if_on(th)\n",
        "\n",
        "    return float(chi2)\n",
        "\n",
        "def chi2_tot_ECTI(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        return np.inf\n",
        "\n",
        "    chi2 = (\n",
        "        float(chi2_SN_ECTI(th))\n",
        "        + float(chi2_BAO_ECTI(th))\n",
        "        + float(chi2_RSD_ECTI(th))\n",
        "        + float(chi2_CMB_ECTI(th))\n",
        "    )\n",
        "\n",
        "    if bool(globals().get(\"USE_KIDS\", False)):\n",
        "        chi2 += float(chi2_KiDS_ECTI(th))\n",
        "\n",
        "    chi2 += _chi2_lensing_if_on(th)\n",
        "\n",
        "    return float(chi2)\n",
        "\n",
        "def lnprob_LCDM(theta):\n",
        "    lp = lnprior_LCDM(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    chi2 = chi2_tot_LCDM(theta)\n",
        "    if not np.isfinite(chi2):\n",
        "        return -np.inf\n",
        "    return lp - 0.5 * chi2\n",
        "\n",
        "def lnprob_ECTI(theta):\n",
        "    lp = lnprior_ECTI(theta)\n",
        "    if not np.isfinite(lp):\n",
        "        return -np.inf\n",
        "    chi2 = chi2_tot_ECTI(theta)\n",
        "    if not np.isfinite(chi2):\n",
        "        return -np.inf\n",
        "    return lp - 0.5 * chi2\n",
        "\n",
        "print(\"✔ Priors + lnprob ready (FAIR k=5): lnprob_LCDM, lnprob_ECTI\",\n",
        "      f\"| USE_PLANCK_LENSING={bool(globals().get('USE_PLANCK_LENSING', False))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yox54pNYkkh4"
      },
      "source": [
        "# 7) Walker initialization and χ² diagnostics\n",
        "\n",
        "This section defines:\n",
        "\n",
        "• Stable initialization centers for ΛCDM and ECTI-P  \n",
        "• Randomized walker starting points (p0)  \n",
        "• χ² breakdown helpers used after each MCMC run  \n",
        "\n",
        "This cell **does NOT launch any MCMC** and is safe under\n",
        "\"Restart runtime & Run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AYxWo6vl6Lv"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7.1) Walker initialization and χ² diagnostics (FAIR k=5)\n",
        "#    θ5 = (H0, Om0, σ8, M, wb) for BOTH ΛCDM and ECTI\n",
        "#\n",
        "#    UPDATED:\n",
        "#      - chi2_breakdown_* includes LENS block when USE_PLANCK_LENSING=True\n",
        "#      - Keeps return signature: (parts_dict, total_float)\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "SIGMA8_INIT = 0.811\n",
        "WB_INIT     = 0.02237\n",
        "\n",
        "# Stable init centers\n",
        "INIT_CENTER = np.array([67.4, 0.315, SIGMA8_INIT, -19.4, WB_INIT], dtype=float)\n",
        "\n",
        "def p0_theta5(nwalkers, scale=None, rng=None):\n",
        "    \"\"\"\n",
        "    Gaussian cloud around INIT_CENTER with explicit per-parameter scales.\n",
        "    Returns p0 shape (nwalkers, 5).\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    if scale is None:\n",
        "        # conservative, auditable widths (tune if needed)\n",
        "        scale = np.array([2.0, 0.05, 0.06, 0.25, 0.002], dtype=float)\n",
        "\n",
        "    scale = np.asarray(scale, dtype=float).ravel()\n",
        "    if scale.size != 5:\n",
        "        raise ValueError(\"scale must be length 5 for θ5.\")\n",
        "\n",
        "    p0 = INIT_CENTER[None, :] + rng.normal(size=(int(nwalkers), 5)) * scale[None, :]\n",
        "    return np.asarray(p0, dtype=float)\n",
        "\n",
        "def _chi2_lensing_if_on(theta):\n",
        "    if not bool(globals().get(\"USE_PLANCK_LENSING\", False)):\n",
        "        return 0.0\n",
        "    if \"chi2_PLANCK_LENSING\" not in globals():\n",
        "        raise NameError(\"USE_PLANCK_LENSING=True but chi2_PLANCK_LENSING is not defined (Cell 5.6 missing).\")\n",
        "    return float(globals()[\"chi2_PLANCK_LENSING\"](theta))\n",
        "\n",
        "def chi2_breakdown_LCDM(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(\"chi2_breakdown_LCDM expects θ5.\")\n",
        "    parts = {\n",
        "        \"SN\":   float(chi2_SN_LCDM(th)),\n",
        "        \"BAO\":  float(chi2_BAO_LCDM(th)),\n",
        "        \"RSD\":  float(chi2_RSD_LCDM(th)),\n",
        "        \"KiDS\": float(chi2_KiDS_LCDM(th)) if bool(globals().get(\"USE_KIDS\", False)) else 0.0,\n",
        "        \"CMB\":  float(chi2_CMB_LCDM(th)),\n",
        "        \"LENS\": float(_chi2_lensing_if_on(th)) if bool(globals().get(\"USE_PLANCK_LENSING\", False)) else 0.0,\n",
        "    }\n",
        "    return parts, float(sum(parts.values()))\n",
        "\n",
        "def chi2_breakdown_ECTI(theta):\n",
        "    th = np.asarray(theta, dtype=float).ravel()\n",
        "    if th.size != 5:\n",
        "        raise ValueError(\"chi2_breakdown_ECTI expects θ5.\")\n",
        "    parts = {\n",
        "        \"SN\":   float(chi2_SN_ECTI(th)),\n",
        "        \"BAO\":  float(chi2_BAO_ECTI(th)),\n",
        "        \"RSD\":  float(chi2_RSD_ECTI(th)),\n",
        "        \"KiDS\": float(chi2_KiDS_ECTI(th)) if bool(globals().get(\"USE_KIDS\", False)) else 0.0,\n",
        "        \"CMB\":  float(chi2_CMB_ECTI(th)),\n",
        "        \"LENS\": float(_chi2_lensing_if_on(th)) if bool(globals().get(\"USE_PLANCK_LENSING\", False)) else 0.0,\n",
        "    }\n",
        "    return parts, float(sum(parts.values()))\n",
        "\n",
        "# Quick deterministic diagnostics at INIT_CENTER (does NOT require MCMC)\n",
        "try:\n",
        "    parts_L, tot_L = chi2_breakdown_LCDM(INIT_CENTER)\n",
        "    parts_E, tot_E = chi2_breakdown_ECTI(INIT_CENTER)\n",
        "    print(\"--- χ² breakdown @ INIT_CENTER (FAIR k=5) ---\")\n",
        "    print(\"ΛCDM :\", parts_L, \"Total=\", tot_L)\n",
        "    print(\"ECTI :\", parts_E, \"Total=\", tot_E)\n",
        "except Exception as _e:\n",
        "    print(\"⚠ χ² breakdown diagnostic failed (check data availability):\", repr(_e))\n",
        "\n",
        "print(\"✔ 10bis ready: p0_theta5 + chi2_breakdown_* (FAIR k=5)\",\n",
        "      f\"| USE_PLANCK_LENSING={bool(globals().get('USE_PLANCK_LENSING', False))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbRvjsMZl_39",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 8) MCMC runner (multi-run, autosave, protected)\n",
        "\n",
        "This section executes the full MCMC campaign for ΛCDM and ECTI-P.\n",
        "\n",
        "Safety features:\n",
        "• Controlled by RUN_MCMC flag\n",
        "• One directory per campaign\n",
        "• One file per independent run\n",
        "• Autosave after each run\n",
        "• Safe under \"Run all\" when RUN_MCMC = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJNh4Yvb-UF0"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8.1) MCMC runner — FAIR k=5 (θ5 = H0, Om0, σ8, M, wb)\n",
        "#      Planck compressed priors: (R + ℓA + ωb) everywhere\n",
        "#      Lensing-derived 1D prior: optional via USE_PLANCK_LENSING (no new params)\n",
        "#\n",
        "#      ✅ ORDER: PAIRS (LCDM run i -> ECTI run i -> Δχ² + Δblocks)\n",
        "#      ✅ LOGS : NO progress bar, print every PRINT_EVERY steps with ETA\n",
        "#      ✅ IO   : HDF5 autosave + resume (safe resume with fallback)\n",
        "# ============================================================\n",
        "\n",
        "import os, time, json\n",
        "import numpy as np\n",
        "import emcee\n",
        "from emcee.backends import HDFBackend\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------------------------\n",
        "# Required symbols (strict)\n",
        "# ---------------------------\n",
        "_required = [\n",
        "    \"RUN_MCMC\", \"N_WALKERS\", \"N_STEPS\", \"N_BURN\",\n",
        "    \"lnprob_LCDM\", \"lnprob_ECTI\",\n",
        "    \"lnprior_LCDM\", \"lnprior_ECTI\",\n",
        "    \"chi2_tot_LCDM\", \"chi2_tot_ECTI\",\n",
        "    \"chi2_breakdown_LCDM\", \"chi2_breakdown_ECTI\",\n",
        "]\n",
        "_missing = [n for n in _required if n not in globals()]\n",
        "if _missing:\n",
        "    raise NameError(\"Missing required symbols before Cell 8.1: \" + \", \".join(_missing))\n",
        "\n",
        "# ---------------------------\n",
        "# Config (single source of truth)\n",
        "# ---------------------------\n",
        "NDIM        = 5\n",
        "N_WALKERS   = int(N_WALKERS)\n",
        "N_STEPS     = int(N_STEPS)\n",
        "N_BURN      = int(N_BURN)\n",
        "THIN        = int(globals().get(\"THIN\", 1))\n",
        "MOVE_A      = float(globals().get(\"MOVE_A\", 2.0))\n",
        "PRINT_EVERY = int(globals().get(\"PRINT_EVERY\", 1000))\n",
        "N_RUNS      = int(globals().get(\"N_RUNS\", 3))\n",
        "USE_KIDS    = bool(globals().get(\"USE_KIDS\", False))\n",
        "USE_PLANCK_LENSING = bool(globals().get(\"USE_PLANCK_LENSING\", False))\n",
        "\n",
        "if N_WALKERS <= 2 * NDIM:\n",
        "    raise ValueError(f\"N_WALKERS must be > 2*NDIM (=10) for StretchMove. Got {N_WALKERS}.\")\n",
        "if N_STEPS <= N_BURN + 10:\n",
        "    raise ValueError(\"N_STEPS must be > N_BURN + 10\")\n",
        "if THIN < 1:\n",
        "    raise ValueError(\"THIN must be >= 1\")\n",
        "if N_RUNS < 1:\n",
        "    raise ValueError(\"N_RUNS must be >= 1\")\n",
        "if PRINT_EVERY < 1:\n",
        "    raise ValueError(\"PRINT_EVERY must be >= 1\")\n",
        "\n",
        "print(\"====================================================\")\n",
        "print(\"▶ MCMC runner (FAIR k=5) — PAIRED ORDER\")\n",
        "print(f\"N_RUNS            = {N_RUNS}\")\n",
        "print(f\"N_WALKERS         = {N_WALKERS}\")\n",
        "print(f\"N_STEPS           = {N_STEPS}\")\n",
        "print(f\"N_BURN            = {N_BURN}\")\n",
        "print(f\"THIN              = {THIN}\")\n",
        "print(f\"MOVE_A            = {MOVE_A}\")\n",
        "print(f\"PRINT_EVERY       = {PRINT_EVERY}\")\n",
        "print(f\"USE_KIDS          = {USE_KIDS}  (consumed inside chi2_* via globals())\")\n",
        "print(f\"USE_PLANCK_LENSING= {USE_PLANCK_LENSING}\")\n",
        "print(\"====================================================\")\n",
        "\n",
        "if not RUN_MCMC:\n",
        "    print(\"⏭ RUN_MCMC=False → skipping MCMC runs (safe Run All).\")\n",
        "else:\n",
        "    # ---------------------------\n",
        "    # Campaign directory (k5 naming)\n",
        "    # ---------------------------\n",
        "    CHAINS_ROOT = globals().get(\"CHAINS_ROOT\", globals().get(\"CHAINS_DIR\", \"ECTI_runs/chains_sigma8free\"))\n",
        "    os.makedirs(CHAINS_ROOT, exist_ok=True)\n",
        "\n",
        "    _CAMPAIGN_POINTER = os.path.join(CHAINS_ROOT, \"LAST_CAMPAIGN_DIR_LCDM_vs_ECTI_k5.txt\")\n",
        "\n",
        "    CAMPAIGN_DIR = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "    if (CAMPAIGN_DIR is None) or (not isinstance(CAMPAIGN_DIR, str)) or (len(CAMPAIGN_DIR.strip()) == 0) or (not os.path.isdir(CAMPAIGN_DIR)):\n",
        "        if os.path.exists(_CAMPAIGN_POINTER):\n",
        "            try:\n",
        "                with open(_CAMPAIGN_POINTER, \"r\", encoding=\"utf-8\") as f:\n",
        "                    _cand = f.read().strip()\n",
        "                if _cand and os.path.isdir(_cand):\n",
        "                    CAMPAIGN_DIR = _cand\n",
        "            except Exception:\n",
        "                CAMPAIGN_DIR = None\n",
        "\n",
        "    if (CAMPAIGN_DIR is None) or (not isinstance(CAMPAIGN_DIR, str)) or (len(CAMPAIGN_DIR.strip()) == 0) or (not os.path.isdir(CAMPAIGN_DIR)):\n",
        "        CAMPAIGN_DIR = os.path.join(\n",
        "            CHAINS_ROOT,\n",
        "            f\"campaign_LCDM_vs_ECTI_k5_{N_STEPS:05d}_{N_RUNS}pairs_FAIRk5_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        )\n",
        "\n",
        "    os.makedirs(CAMPAIGN_DIR, exist_ok=True)\n",
        "    globals()[\"CAMPAIGN_DIR\"] = CAMPAIGN_DIR\n",
        "\n",
        "    try:\n",
        "        with open(_CAMPAIGN_POINTER, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(CAMPAIGN_DIR)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    FIG_DIR  = os.path.join(CAMPAIGN_DIR, \"figures\")\n",
        "    TAB_DIR  = os.path.join(CAMPAIGN_DIR, \"tables\")\n",
        "    POST_DIR = os.path.join(CAMPAIGN_DIR, \"posteriors\")\n",
        "    META_DIR = os.path.join(CAMPAIGN_DIR, \"meta\")\n",
        "    os.makedirs(FIG_DIR, exist_ok=True)\n",
        "    os.makedirs(TAB_DIR, exist_ok=True)\n",
        "    os.makedirs(POST_DIR, exist_ok=True)\n",
        "    os.makedirs(META_DIR, exist_ok=True)\n",
        "\n",
        "    print(\"✔ CAMPAIGN_DIR =\", CAMPAIGN_DIR)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Save campaign config (trace + lensing constants)\n",
        "    # ---------------------------\n",
        "    _meta_cfg = dict(\n",
        "        NDIM=NDIM, N_WALKERS=N_WALKERS, N_STEPS=N_STEPS, N_BURN=N_BURN,\n",
        "        THIN=THIN, MOVE_A=MOVE_A, PRINT_EVERY=PRINT_EVERY,\n",
        "        USE_KIDS=USE_KIDS, USE_PLANCK_LENSING=USE_PLANCK_LENSING,\n",
        "        N_RUNS=N_RUNS,\n",
        "        timestamp=datetime.now().isoformat(),\n",
        "        order=\"PAIRED (LCDM_i then ECTI_i)\",\n",
        "        emcee_version=getattr(emcee, \"__version__\", \"unknown\"),\n",
        "        LENSING=dict(\n",
        "            USE=bool(USE_PLANCK_LENSING),\n",
        "            alpha=float(globals().get(\"LENSING_S8OM025_ALPHA\", np.nan)),\n",
        "            mean=float(globals().get(\"LENSING_S8OM025_MEAN\",  np.nan)),\n",
        "            sigma=float(globals().get(\"LENSING_S8OM025_SIGMA\", np.nan)),\n",
        "            ref=str(globals().get(\"LENSING_S8OM025_REF\", \"\")),\n",
        "        ),\n",
        "    )\n",
        "    try:\n",
        "        with open(os.path.join(META_DIR, \"campaign_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(_meta_cfg, f, indent=2)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # ---------------------------\n",
        "    # Initialization helper (model-prior aware)\n",
        "    # ---------------------------\n",
        "    def init_walkers(center, lnprior_fn, scale=None, seed=42):\n",
        "        rng = np.random.default_rng(int(seed))\n",
        "        center = np.asarray(center, float).ravel()\n",
        "        if center.size != NDIM:\n",
        "            raise ValueError(\"center must be theta5\")\n",
        "\n",
        "        if scale is None:\n",
        "            scale = np.array([0.35, 0.015, 0.040, 0.030, 0.0006], dtype=float)\n",
        "\n",
        "        p0 = center[None, :] + rng.normal(size=(N_WALKERS, NDIM)) * scale[None, :]\n",
        "\n",
        "        eps = 1e-10\n",
        "        p0[:, 0] = np.clip(p0[:, 0], 40.0 + eps, 95.0 - eps)\n",
        "        p0[:, 1] = np.clip(p0[:, 1], 0.05 + eps, 0.60 - eps)\n",
        "        p0[:, 2] = np.clip(p0[:, 2], 0.30 + eps, 1.50 - eps)\n",
        "        p0[:, 3] = np.clip(p0[:, 3], -21.5 + eps, -18.0 - eps)\n",
        "        p0[:, 4] = np.clip(p0[:, 4], 0.005 + eps, 0.040 - eps)\n",
        "\n",
        "        for i in range(N_WALKERS):\n",
        "            tries = 0\n",
        "            while (not np.isfinite(lnprior_fn(p0[i]))) and tries < 8000:\n",
        "                p = center + rng.normal(size=NDIM) * scale\n",
        "                p[0] = np.clip(p[0], 40.0 + eps, 95.0 - eps)\n",
        "                p[1] = np.clip(p[1], 0.05 + eps, 0.60 - eps)\n",
        "                p[2] = np.clip(p[2], 0.30 + eps, 1.50 - eps)\n",
        "                p[3] = np.clip(p[3], -21.5 + eps, -18.0 - eps)\n",
        "                p[4] = np.clip(p[4], 0.005 + eps, 0.040 - eps)\n",
        "                p0[i] = p\n",
        "                tries += 1\n",
        "            if tries >= 8000:\n",
        "                raise RuntimeError(\"Failed to init walkers with finite prior (per-model).\")\n",
        "        return p0\n",
        "\n",
        "    # ---------------------------\n",
        "    # Backend open/reset helper\n",
        "    # ---------------------------\n",
        "    def open_or_reset_backend(h5_path):\n",
        "        backend = HDFBackend(h5_path)\n",
        "\n",
        "        if not os.path.exists(h5_path):\n",
        "            backend.reset(N_WALKERS, NDIM)\n",
        "            return backend, 0, \"NEW\"\n",
        "\n",
        "        try:\n",
        "            it = int(backend.iteration)\n",
        "        except Exception:\n",
        "            backend.reset(N_WALKERS, NDIM)\n",
        "            return backend, 0, \"RESET_CORRUPT\"\n",
        "\n",
        "        if it > 0:\n",
        "            try:\n",
        "                chain0 = backend.get_chain(discard=0, thin=1, flat=False)\n",
        "                if chain0.shape[1] != N_WALKERS or chain0.shape[2] != NDIM:\n",
        "                    backend.reset(N_WALKERS, NDIM)\n",
        "                    return backend, 0, \"RESET_SHAPE\"\n",
        "            except Exception:\n",
        "                backend.reset(N_WALKERS, NDIM)\n",
        "                return backend, 0, \"RESET_READFAIL\"\n",
        "\n",
        "        return backend, it, \"RESUME\" if it > 0 else \"EMPTY\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # ETA helper\n",
        "    # ---------------------------\n",
        "    def _fmt_eta(seconds):\n",
        "        seconds = float(seconds)\n",
        "        if (not np.isfinite(seconds)) or seconds < 0:\n",
        "            return \"?\"\n",
        "        m = int(seconds // 60)\n",
        "        h = m // 60\n",
        "        m = m % 60\n",
        "        return f\"{h}h{m:02d}m\" if h > 0 else f\"{m}m\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # Run one run\n",
        "    # ---------------------------\n",
        "    def run_one(model_tag, rid, lnprob_fn, lnprior_fn, chi2_tot_fn, chi2_breakdown_fn, seed_base):\n",
        "        h5_path = os.path.join(CAMPAIGN_DIR, f\"{model_tag}_run{rid:03d}.h5\")\n",
        "        backend, it0, mode = open_or_reset_backend(h5_path)\n",
        "\n",
        "        # resume-safe init_state\n",
        "        if it0 > 0:\n",
        "            print(f\"\\n▶ {model_tag} run{rid:03d}: RESUMING backend @ iter={it0} ({mode})\")\n",
        "            try:\n",
        "                init_state = backend.get_last_sample()  # emcee State\n",
        "            except Exception:\n",
        "                # ultra-robust fallback: last positions\n",
        "                try:\n",
        "                    p_last = backend.get_chain(discard=0, thin=1, flat=False)[-1, :, :]\n",
        "                    init_state = np.array(p_last, float)\n",
        "                    print(\"  (fallback) resumed from last chain positions (no State).\")\n",
        "                except Exception as e:\n",
        "                    raise RuntimeError(f\"{model_tag} run{rid:03d}: cannot resume (fallback failed): {repr(e)}\")\n",
        "        else:\n",
        "            center = np.asarray(globals().get(\"INIT_CENTER\", [67.4, 0.315, 0.811, -19.4, 0.02237]), float).ravel()\n",
        "            if center.size != NDIM:\n",
        "                raise ValueError(\"INIT_CENTER must be θ5 (size 5) for this runner.\")\n",
        "            seed_used = int(seed_base + rid)\n",
        "            print(f\"\\n▶ {model_tag} run{rid:03d}: NEW init ({mode}) | seed={seed_used}\")\n",
        "            p0 = init_walkers(center, lnprior_fn=lnprior_fn, seed=seed_used)\n",
        "            init_state = p0\n",
        "\n",
        "            try:\n",
        "                with open(os.path.join(META_DIR, f\"{model_tag}_run{rid:03d}_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(dict(model=model_tag, run_id=rid, seed=seed_used, init_center=list(map(float, center))), f, indent=2)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        sampler = emcee.EnsembleSampler(\n",
        "            N_WALKERS, NDIM, lnprob_fn,\n",
        "            backend=backend,\n",
        "            moves=emcee.moves.StretchMove(a=MOVE_A)\n",
        "        )\n",
        "\n",
        "        it_start = int(backend.iteration)\n",
        "        n_remaining = N_STEPS - it_start\n",
        "\n",
        "        if n_remaining <= 0:\n",
        "            print(f\"⏭ {model_tag} run{rid:03d}: already at N_STEPS={N_STEPS}\")\n",
        "        else:\n",
        "            print(f\"▶ {model_tag} run{rid:03d}: running {n_remaining} steps (target N_STEPS={N_STEPS})\")\n",
        "            t0 = time.time()\n",
        "            for _ in sampler.sample(init_state, iterations=n_remaining, progress=False, store=True):\n",
        "                it_abs = it_start + int(sampler.iteration)\n",
        "                if (it_abs % PRINT_EVERY) == 0 or it_abs == N_STEPS:\n",
        "                    dt = time.time() - t0\n",
        "                    done = max(1, it_abs - it_start)\n",
        "                    rate = done / max(1e-12, dt)\n",
        "                    eta_s = (N_STEPS - it_abs) / max(1e-12, rate)\n",
        "                    print(f\"  iter={it_abs}/{N_STEPS} | elapsed={dt/60:.1f} min | eta={_fmt_eta(eta_s)}\")\n",
        "\n",
        "        it_final = int(backend.iteration)\n",
        "        if it_final <= N_BURN:\n",
        "            raise RuntimeError(\n",
        "                f\"{model_tag} run{rid:03d}: backend.iteration={it_final} <= N_BURN={N_BURN}. \"\n",
        "                \"Run not advanced enough to extract post-burn MAP.\"\n",
        "            )\n",
        "\n",
        "        chain = backend.get_chain(discard=N_BURN, thin=THIN, flat=False)\n",
        "        logp  = backend.get_log_prob(discard=N_BURN, thin=THIN, flat=False)\n",
        "\n",
        "        if chain.size == 0 or logp.size == 0:\n",
        "            raise RuntimeError(f\"{model_tag} run{rid:03d}: empty chain/logp after discard (check N_BURN/THIN).\")\n",
        "\n",
        "        ij = np.unravel_index(int(np.nanargmax(logp)), logp.shape)\n",
        "        th_map = np.array(chain[ij[0], ij[1], :], float)\n",
        "\n",
        "        parts, chi2_map_from_parts = chi2_breakdown_fn(th_map)\n",
        "        chi2_map = float(chi2_tot_fn(th_map))\n",
        "\n",
        "        if abs(float(chi2_map) - float(chi2_map_from_parts)) > 1e-8:\n",
        "            raise RuntimeError(\n",
        "                f\"{model_tag} run{rid:03d}: mismatch chi2_tot(MAP)={chi2_map} vs sum(parts)={chi2_map_from_parts}\"\n",
        "            )\n",
        "\n",
        "        if USE_PLANCK_LENSING and (\"LENS\" not in parts):\n",
        "            print(f\"⚠ WARNING: USE_PLANCK_LENSING=True but 'LENS' key not found in parts for {model_tag} run{rid:03d}.\")\n",
        "\n",
        "        print(\"\\n====================================================\")\n",
        "        print(f\"RESULT — {model_tag} run{rid:03d} (post-burn MAP)\")\n",
        "        print(\"theta_MAP =\", np.array2string(th_map, precision=10))\n",
        "        print(f\"chi2_MAP  = {chi2_map:.6f}\")\n",
        "        print(\"parts     =\", parts)\n",
        "        print(\"====================================================\\n\")\n",
        "\n",
        "        return th_map, float(chi2_map), dict(parts)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Execute in PAIRS\n",
        "    # ---------------------------\n",
        "    SEED_BASE_LCDM = int(globals().get(\"SEED_BASE_LCDM\", 20260110))\n",
        "    SEED_BASE_ECTI = int(globals().get(\"SEED_BASE_ECTI\", 20260111))\n",
        "\n",
        "    pairs_summary = []\n",
        "    run_rows = []\n",
        "\n",
        "    def _delta_blocks(partsE, partsL):\n",
        "        keys = [\"SN\",\"BAO\",\"RSD\",\"KiDS\",\"CMB\",\"LENS\",\"TOTAL\"]\n",
        "        return {k: float(partsE.get(k, 0.0)) - float(partsL.get(k, 0.0)) for k in keys}\n",
        "\n",
        "    for rid in range(1, N_RUNS + 1):\n",
        "        print(\"====================================================\")\n",
        "        print(f\"▶ PAIR {rid}/{N_RUNS}\")\n",
        "        print(\"====================================================\")\n",
        "\n",
        "        thL, chiL, partsL = run_one(\n",
        "            \"LCDM_k5\", rid,\n",
        "            lnprob_fn=lnprob_LCDM, lnprior_fn=lnprior_LCDM,\n",
        "            chi2_tot_fn=chi2_tot_LCDM, chi2_breakdown_fn=chi2_breakdown_LCDM,\n",
        "            seed_base=SEED_BASE_LCDM\n",
        "        )\n",
        "        thE, chiE, partsE = run_one(\n",
        "            \"ECTI_k5\", rid,\n",
        "            lnprob_fn=lnprob_ECTI, lnprior_fn=lnprior_ECTI,\n",
        "            chi2_tot_fn=chi2_tot_ECTI, chi2_breakdown_fn=chi2_breakdown_ECTI,\n",
        "            seed_base=SEED_BASE_ECTI\n",
        "        )\n",
        "\n",
        "        dchi = float(chiE - chiL)\n",
        "        dblk = _delta_blocks(partsE, partsL)\n",
        "\n",
        "        dAIC = dchi\n",
        "        dBIC = dchi\n",
        "\n",
        "        print(\"----------------------------------------------------\")\n",
        "        print(f\"PAIR {rid:03d} SUMMARY (MAP)\")\n",
        "        print(f\"LCDM chi2_MAP = {chiL:.6f}\")\n",
        "        print(f\"ECTI chi2_MAP = {chiE:.6f}\")\n",
        "        print(f\"Δχ²_MAP (ECTI−LCDM) = {dchi:+.6f}  (ΔAIC=ΔBIC=Δχ² for same-k)\")\n",
        "        print(\"Δχ² blocks (ECTI−LCDM):\")\n",
        "        for k in [\"SN\",\"BAO\",\"RSD\",\"KiDS\",\"CMB\",\"LENS\",\"TOTAL\"]:\n",
        "            print(f\"  Δ{k:5s} = {dblk.get(k, 0.0):+.6f}\")\n",
        "        print(\"----------------------------------------------------\\n\")\n",
        "\n",
        "        pairs_summary.append(dict(\n",
        "            rid=int(rid),\n",
        "            LCDM=dict(theta_MAP=thL.tolist(), chi2_MAP=float(chiL), parts=partsL),\n",
        "            ECTI=dict(theta_MAP=thE.tolist(), chi2_MAP=float(chiE), parts=partsE),\n",
        "            dchi2_MAP=float(dchi),\n",
        "            dAIC=float(dAIC),\n",
        "            dBIC=float(dBIC),\n",
        "            dchi2_blocks=dblk,\n",
        "        ))\n",
        "\n",
        "        run_rows.append((rid, chiL, chiE, dchi, dAIC, dBIC))\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(META_DIR, \"pairs_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(dict(pairs=pairs_summary), f, indent=2)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"FINAL SUMMARY — FAIR k=5 (PAIRED runs)\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\" run_id   chi2_LCDM     chi2_ECTI       dchi2       dAIC       dBIC\")\n",
        "    for (rid, chiL, chiE, dchi, dAIC, dBIC) in run_rows:\n",
        "        print(f\"{rid:6d} {chiL:11.6f} {chiE:12.6f} {dchi:11.6f} {dAIC:11.6f} {dBIC:11.6f}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    rid_ref, chiL_ref, chiE_ref, dchi_ref, *_ = run_rows[-1]\n",
        "    print(f\"REFERENCE = LAST common run_id = run{rid_ref:03d}\")\n",
        "    print(f\"Δχ²_ref (ECTI−LCDM) = {dchi_ref:+.6f}\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    globals()[\"PAIRS_SUMMARY_K5\"] = pairs_summary\n",
        "    print(\"✔ Stored: PAIRS_SUMMARY_K5\")\n",
        "    print(\"✔ MCMC campaign completed:\", CAMPAIGN_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmdsagziYieR"
      },
      "source": [
        "# 9) Convergence diagnostics — Campaign (R̂, τ_int, ESS)\n",
        "\n",
        "This section performs **global convergence diagnostics** for the full MCMC campaign,\n",
        "using the **automatically saved HDF5 backends** (`LCDM_run*.h5`, `ECTI_run*.h5`).\n",
        "\n",
        "### Goals\n",
        "\n",
        "For each model (ΛCDM and ECTI-P), this step is designed to:\n",
        "\n",
        "- Assess **inter-chain convergence** using the **Gelman–Rubin statistic (R̂)**\n",
        "- Estimate the **integrated autocorrelation time** τ_int for each parameter\n",
        "- Compute the **Effective Sample Size (ESS)** as a measure of statistical robustness\n",
        "- Ensure convergence is not an artifact of a single run\n",
        "\n",
        "This analysis is **mandatory before any final interpretation or publication**.\n",
        "\n",
        "---\n",
        "\n",
        "### Methodology\n",
        "\n",
        "- Each run (`run1`, `run2`, `run3`) is treated as an **independent MCMC chain**\n",
        "- A **global burn-in (`N_BURN`)** is removed prior to diagnostics\n",
        "- Chains are read **directly from the HDF5 backends**\n",
        "- R̂ is computed **across independent runs**, not across walkers\n",
        "- τ_int and ESS are estimated **per parameter**, then aggregated across runs\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation criteria\n",
        "\n",
        "**Gelman–Rubin (R̂)**  \n",
        "- R̂ < 1.01 → excellent convergence  \n",
        "- R̂ < 1.05 → acceptable convergence  \n",
        "- R̂ ≥ 1.05 → not converged (longer runs required)\n",
        "\n",
        "**Effective Sample Size (ESS)**  \n",
        "- ESS > 300 → minimum acceptable  \n",
        "- ESS > 1000 → robust (publication-level)\n",
        "\n",
        "**Autocorrelation time (τ_int)**  \n",
        "- N_steps / τ_int ≫ 50 recommended  \n",
        "- Large τ_int indicates strongly correlated chains\n",
        "\n",
        "---\n",
        "\n",
        "### Important notes\n",
        "\n",
        "- This section is **fully HPC-compatible** (CPU, MPI, Slurm, cloud environments)\n",
        "- No likelihood recomputation is performed\n",
        "- Failure to estimate τ_int usually indicates insufficient chain length\n",
        "- This step **does not replace posterior analysis**, it validates it\n",
        "\n",
        "---\n",
        "\n",
        "### Expected outcome\n",
        "\n",
        "At the end of this section:\n",
        "\n",
        "- The campaign convergence status is clearly established\n",
        "- The adequacy of the chosen number of steps (e.g. 10,000) is quantified\n",
        "- Posterior results can be interpreted **with statistical confidence**\n",
        "\n",
        "➡️ Subsequent cells (10-13) can then be analyzed safely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxg6Z8DjXZ2C",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9.1) Convergence diagnostics (Rhat / tau_int / ESS) — FAIR k=5\n",
        "#   - marche sur familles LCDM_k5 / ECTI_k5 dans CAMPAIGN_DIR\n",
        "#   - calcule split-Rhat (walkers = chains)\n",
        "#   - tau_int via emcee.autocorr.integrated_time (si possible)\n",
        "#   - export CSV + JSON\n",
        "# ============================================================\n",
        "\n",
        "import os, re, json, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from emcee.backends import HDFBackend\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get('RUN_MCMC', True))\n",
        "_cd = globals().get('CAMPAIGN_DIR', None)\n",
        "_SKIP_POST = (not _RUN_MCMC) or (not isinstance(_cd, str)) or (not os.path.isdir(_cd))\n",
        "if _SKIP_POST:\n",
        "    print('⏭ Skipping Cell 9.1 (Convergence diagnostics): RUN_MCMC=False or CAMPAIGN_DIR missing/invalid.')\n",
        "else:\n",
        "    if not any(f.lower().endswith('.h5') for f in os.listdir(_cd)):\n",
        "        _SKIP_POST = True\n",
        "        print('⏭ Skipping Cell 9.1 (Convergence diagnostics): no .h5 files found in CAMPAIGN_DIR=' + str(_cd))\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    # Rehydrate TAB_DIR if needed (Run-All safe)\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(_cd, \"tables\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    # Rehydrate THIN if needed (safe default)\n",
        "    if \"THIN\" not in globals():\n",
        "        globals()[\"THIN\"] = 1\n",
        "\n",
        "    _required = [\"CAMPAIGN_DIR\", \"N_BURN\", \"THIN\", \"TAB_DIR\"]\n",
        "    _missing = [k for k in _required if k not in globals()]\n",
        "    if _missing:\n",
        "        raise NameError(\"Missing required symbols before Cell 9.1: \" + \", \".join(_missing))\n",
        "\n",
        "    def _safe_backend(h5_path, tries=10, wait_s=0.6):\n",
        "        last = None\n",
        "        for _ in range(tries):\n",
        "            try:\n",
        "                return HDFBackend(h5_path)\n",
        "            except OSError as e:\n",
        "                last = e\n",
        "                time.sleep(wait_s)\n",
        "        raise last\n",
        "\n",
        "    def _extract_runid(path):\n",
        "        m = re.search(r\"_run(\\d+)\\.h5$\", os.path.basename(path), flags=re.IGNORECASE)\n",
        "        return int(m.group(1)) if m else None\n",
        "\n",
        "    def _list_family_files(campaign_dir):\n",
        "        files = [f for f in os.listdir(campaign_dir) if f.lower().endswith(\".h5\")]\n",
        "        pat = re.compile(r\"^(.*)_run(\\d+)\\.h5$\", re.IGNORECASE)\n",
        "        buckets = {}\n",
        "        for f in files:\n",
        "            m = pat.match(f)\n",
        "            if not m:\n",
        "                continue\n",
        "            fam = m.group(1).upper()\n",
        "            rid = int(m.group(2))\n",
        "            if not (fam.startswith(\"LCDM\") or fam.startswith(\"ECTI\")):\n",
        "                continue\n",
        "            buckets.setdefault(fam, []).append((rid, os.path.join(campaign_dir, f)))\n",
        "        for fam in list(buckets.keys()):\n",
        "            buckets[fam] = [p for _, p in sorted(buckets[fam], key=lambda x: x[0])]\n",
        "        return buckets\n",
        "\n",
        "    def _choose_family(buckets, kind=\"LCDM\"):\n",
        "        keys = sorted([k for k in buckets.keys() if k.startswith(kind.upper())])\n",
        "        if not keys:\n",
        "            return None\n",
        "        if kind.upper() == \"LCDM\":\n",
        "            for pref in [\"LCDM_K5\"]:\n",
        "                for k in keys:\n",
        "                    if pref in k:\n",
        "                        return k\n",
        "            return keys[0]\n",
        "        else:\n",
        "            for pref in [\"ECTI_K5\"]:\n",
        "                for k in keys:\n",
        "                    if pref in k:\n",
        "                        return k\n",
        "            return keys[0]\n",
        "\n",
        "    def _split_rhat(x):\n",
        "        x = np.asarray(x, dtype=float)\n",
        "        if x.ndim != 2:\n",
        "            raise ValueError(\"_split_rhat expects 2D (nsteps, nchains)\")\n",
        "        n, m = x.shape\n",
        "        if n < 20 or m < 2:\n",
        "            return np.nan\n",
        "        n2 = n // 2\n",
        "        if n2 < 10:\n",
        "            return np.nan\n",
        "        x = x[:2*n2, :]\n",
        "        first = x[:n2, :]\n",
        "        second = x[n2:2*n2, :]\n",
        "        y = np.vstack([first.T, second.T])  # (2m, n2)\n",
        "        M = y.shape[0]\n",
        "        N = y.shape[1]\n",
        "        chain_means = np.mean(y, axis=1)\n",
        "        chain_vars  = np.var(y, axis=1, ddof=1)\n",
        "        W = np.mean(chain_vars)\n",
        "        B = N * np.var(chain_means, ddof=1)\n",
        "        if not np.isfinite(W) or not np.isfinite(B) or W <= 0:\n",
        "            return np.nan\n",
        "        var_hat = (N - 1)/N * W + (1/N) * B\n",
        "        return float(np.sqrt(var_hat / W))\n",
        "\n",
        "    def _tau_int_emcee(y_1d):\n",
        "        try:\n",
        "            from emcee.autocorr import integrated_time\n",
        "            t = integrated_time(np.asarray(y_1d, float), tol=50, quiet=True)\n",
        "            t = np.atleast_1d(t).astype(float)\n",
        "            return float(t[0]) if np.isfinite(t[0]) else np.nan\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "    def _diagnose_file(h5_path, burn, thin=1):\n",
        "        b = _safe_backend(h5_path)\n",
        "        it = int(getattr(b, \"iteration\", b.get_chain().shape[0]))\n",
        "        burn_eff = int(min(burn, max(it - 1, 0)))\n",
        "        chain = b.get_chain(discard=burn_eff, thin=int(thin), flat=False)\n",
        "        if chain.shape[0] < 2:\n",
        "            raise RuntimeError(f\"Too short post-burn chain in {os.path.basename(h5_path)}\")\n",
        "        nstep, nwalk, ndim = chain.shape\n",
        "\n",
        "        rhats = []\n",
        "        taus  = []\n",
        "        ess   = []\n",
        "        for j in range(ndim):\n",
        "            x = chain[:, :, j]  # (nstep, nwalk)\n",
        "            rhat = _split_rhat(x)\n",
        "            rhats.append(rhat)\n",
        "\n",
        "            series = np.mean(x, axis=1)\n",
        "            tau = _tau_int_emcee(series)\n",
        "            taus.append(tau)\n",
        "\n",
        "            if np.isfinite(tau) and tau > 0:\n",
        "                ess_j = (nstep * nwalk) / tau\n",
        "            else:\n",
        "                ess_j = np.nan\n",
        "            ess.append(float(ess_j))\n",
        "\n",
        "        return dict(\n",
        "            file=os.path.basename(h5_path),\n",
        "            path=h5_path,\n",
        "            iteration=it,\n",
        "            burn=burn_eff,\n",
        "            post_steps=int(nstep),\n",
        "            nwalkers=int(nwalk),\n",
        "            ndim=int(ndim),\n",
        "            rhat_max=float(np.nanmax(rhats)),\n",
        "            rhat_vec=[float(x) if np.isfinite(x) else None for x in rhats],\n",
        "            tau_med=float(np.nanmedian(taus)),\n",
        "            tau_vec=[float(x) if np.isfinite(x) else None for x in taus],\n",
        "            ess_med=float(np.nanmedian(ess)),\n",
        "            ess_vec=[float(x) if np.isfinite(x) else None for x in ess],\n",
        "        )\n",
        "\n",
        "    buckets = _list_family_files(CAMPAIGN_DIR)\n",
        "    famL = _choose_family(buckets, \"LCDM\")\n",
        "    famE = _choose_family(buckets, \"ECTI\")\n",
        "    if famL is None or famE is None:\n",
        "        raise RuntimeError(f\"Families not found in CAMPAIGN_DIR. Found={sorted(buckets.keys())}\")\n",
        "\n",
        "    files = [(famL, p) for p in buckets[famL]] + [(famE, p) for p in buckets[famE]]\n",
        "\n",
        "    rows = []\n",
        "    meta = {\"campaign_dir\": CAMPAIGN_DIR, \"families\": {\"LCDM\": famL, \"ECTI\": famE}, \"results\": []}\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"9.1) Convergence diagnostics — FAIR k=5\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"CAMPAIGN_DIR =\", CAMPAIGN_DIR)\n",
        "    print(\"LCDM family  =\", famL)\n",
        "    print(\"ECTI family  =\", famE)\n",
        "    print(\"burn/thin    =\", int(N_BURN), \"/\", int(THIN))\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    for fam, path in files:\n",
        "        rid = _extract_runid(path)\n",
        "        d = _diagnose_file(path, burn=int(N_BURN), thin=int(THIN))\n",
        "        d[\"family\"] = fam\n",
        "        d[\"run_id\"] = rid\n",
        "        rows.append({\n",
        "            \"family\": fam,\n",
        "            \"run_id\": rid,\n",
        "            \"file\": d[\"file\"],\n",
        "            \"iteration\": d[\"iteration\"],\n",
        "            \"burn\": d[\"burn\"],\n",
        "            \"post_steps\": d[\"post_steps\"],\n",
        "            \"nwalkers\": d[\"nwalkers\"],\n",
        "            \"ndim\": d[\"ndim\"],\n",
        "            \"rhat_max\": d[\"rhat_max\"],\n",
        "            \"tau_med\": d[\"tau_med\"],\n",
        "            \"ess_med\": d[\"ess_med\"],\n",
        "        })\n",
        "        meta[\"results\"].append(d)\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values([\"family\", \"run_id\"]).reset_index(drop=True)\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    csv_path = os.path.join(TAB_DIR, \"convergence_diagnostics_k5.csv\")\n",
        "    json_path = os.path.join(TAB_DIR, \"convergence_diagnostics_k5.json\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    globals()[\"CONV_DF_9.1\"] = df\n",
        "    globals()[\"CONV_META_9.1\"] = meta\n",
        "\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"✔ Saved:\", json_path)\n",
        "    print(\"✔ Stored: CONV_DF_9.1, CONV_META_9.1\")\n",
        "    print(\"====================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVrZ0I5FHqRY"
      },
      "source": [
        "## Limitations and scope\n",
        "\n",
        "This notebook implements a late-time phenomenological extension (ECTI-P) and compares it to $\\Lambda$CDM under a FAIR $k=5$ parameterization.\n",
        "\n",
        "- CMB information is included via compressed distance priors ($R$, $\\ell_A$, $\\omega_b$), not the full CMB power-spectrum likelihood.\n",
        "- The analysis does not model non-linear structure formation, baryonic feedback, or detailed growth systematics beyond the implemented RSD ($f\\sigma_8$) and a KiDS $S_8$ prior.\n",
        "- The goal is not to claim a replacement of the standard model, but to provide a clean, reproducible baseline to assess whether deeper tests (full CMB likelihood, extended modelling) are warranted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsw5PeDdmywP"
      },
      "source": [
        "# 10) Final results — χ², Δχ², AIC, BIC (publication summary)\n",
        "\n",
        "This section summarizes the **final statistical comparison**\n",
        "between ΛCDM and ECTI-P after convergence has been validated.\n",
        "\n",
        "For each model we report:\n",
        "• Best-fit χ² (total)\n",
        "• Δχ² relative to ΛCDM\n",
        "• AIC and ΔAIC\n",
        "• BIC and ΔBIC\n",
        "\n",
        "Definitions:\n",
        "AIC  = χ² + 2k  \n",
        "BIC  = χ² + k ln(N)\n",
        "\n",
        "where:\n",
        "k = number of free parameters  \n",
        "N = total number of data points\n",
        "\n",
        "This table is the **core quantitative result** of the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs7c-VX48xdw"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.1) Final results (MAP/χ²/AIC/BIC) — FAIR k=5 (LAST RUN POLICY)\n",
        "#   - détecte familles LCDM_k5 / ECTI_k5 dans CAMPAIGN_DIR\n",
        "#   - extrait MAP par run + choisit le LAST run (run_id max commun) comme référence\n",
        "#   - calcule Δχ², ΔAIC, ΔBIC (k identique: k=5 vs k=5)\n",
        "#   - REFEREE-PROOF:\n",
        "#       * N_DATA inclut LENS=1 si USE_PLANCK_LENSING=True (cohérence BIC vs χ²_total)\n",
        "#       * meta exporte la policy + toggles\n",
        "#   - exporte: FINAL_DF_10.1, BEST_10.1, N_DATA_10.1, K_PARAMS_10.1\n",
        "# ============================================================\n",
        "\n",
        "import os, re, time, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from emcee.backends import HDFBackend\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get('RUN_MCMC', True))\n",
        "_cd = globals().get('CAMPAIGN_DIR', None)\n",
        "_SKIP_POST = (not _RUN_MCMC) or (not isinstance(_cd, str)) or (not os.path.isdir(_cd))\n",
        "if _SKIP_POST:\n",
        "    print('⏭ Skipping Cell 10.1 (Final results): RUN_MCMC=False or CAMPAIGN_DIR missing/invalid.')\n",
        "else:\n",
        "    if not any(f.lower().endswith('.h5') for f in os.listdir(_cd)):\n",
        "        _SKIP_POST = True\n",
        "        print('⏭ Skipping Cell 10.1 (Final results): no .h5 files found in CAMPAIGN_DIR=' + str(_cd))\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    # Rehydrate output dirs (Run-All safe)\n",
        "    TAB_DIR_LOCAL  = globals().get(\"TAB_DIR\",  os.path.join(_cd, \"tables\"))\n",
        "    FIG_DIR_LOCAL  = globals().get(\"FIG_DIR\",  os.path.join(_cd, \"figures\"))\n",
        "    POST_DIR_LOCAL = globals().get(\"POST_DIR\", os.path.join(_cd, \"posteriors\"))\n",
        "    META_DIR_LOCAL = globals().get(\"META_DIR\", os.path.join(_cd, \"meta\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(FIG_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(POST_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(META_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"]  = TAB_DIR_LOCAL\n",
        "    globals()[\"FIG_DIR\"]  = FIG_DIR_LOCAL\n",
        "    globals()[\"POST_DIR\"] = POST_DIR_LOCAL\n",
        "    globals()[\"META_DIR\"] = META_DIR_LOCAL\n",
        "\n",
        "    _required = [\n",
        "        \"CAMPAIGN_DIR\", \"N_BURN\",\n",
        "        \"chi2_breakdown_LCDM\", \"chi2_breakdown_ECTI\",\n",
        "    ]\n",
        "    _missing = [k for k in _required if k not in globals()]\n",
        "    if _missing:\n",
        "        raise NameError(\"Missing required symbols before Cell 10.1: \" + \", \".join(_missing))\n",
        "\n",
        "    # Prefer explicit N_DATA from data-loading cells if available\n",
        "    N_SN   = int(globals().get(\"N_SN\", 1701))\n",
        "    N_BAO  = int(globals().get(\"N_BAO\", 12))\n",
        "    N_RSD  = int(globals().get(\"N_RSD\", 7))\n",
        "    N_KIDS = 1 if bool(globals().get(\"USE_KIDS\", False)) else 0\n",
        "    N_CMB  = 3\n",
        "    N_LENS = 1 if bool(globals().get(\"USE_PLANCK_LENSING\", False)) else 0\n",
        "    N_DATA = int(N_SN + N_BAO + N_RSD + N_KIDS + N_CMB + N_LENS)\n",
        "\n",
        "    K_PARAMS = 5  # FAIR k=5 in this notebook\n",
        "\n",
        "    globals()[\"N_DATA_10.1\"] = N_DATA\n",
        "    globals()[\"K_PARAMS_10.1\"] = K_PARAMS\n",
        "\n",
        "    def _extract_runid(path):\n",
        "        m = re.search(r\"_run(\\d+)\\.h5$\", os.path.basename(path), flags=re.IGNORECASE)\n",
        "        return int(m.group(1)) if m else None\n",
        "\n",
        "    def _list_family_files(campaign_dir):\n",
        "        files = [f for f in os.listdir(campaign_dir) if f.lower().endswith(\".h5\")]\n",
        "        pat = re.compile(r\"^(.*)_run(\\d+)\\.h5$\", re.IGNORECASE)\n",
        "        buckets = {}\n",
        "        for f in files:\n",
        "            m = pat.match(f)\n",
        "            if not m:\n",
        "                continue\n",
        "            fam = m.group(1).upper()\n",
        "            rid = int(m.group(2))\n",
        "            if not (fam.startswith(\"LCDM\") or fam.startswith(\"ECTI\")):\n",
        "                continue\n",
        "            buckets.setdefault(fam, []).append((rid, os.path.join(campaign_dir, f)))\n",
        "        for fam in list(buckets.keys()):\n",
        "            buckets[fam] = [p for _, p in sorted(buckets[fam], key=lambda x: x[0])]\n",
        "        return buckets\n",
        "\n",
        "    def _choose_family(buckets, kind=\"LCDM\"):\n",
        "        keys = sorted([k for k in buckets.keys() if k.startswith(kind.upper())])\n",
        "        if not keys:\n",
        "            return None\n",
        "        pref = f\"{kind.upper()}_K5\"\n",
        "        for k in keys:\n",
        "            if pref in k:\n",
        "                return k\n",
        "        return keys[0]\n",
        "\n",
        "    def _map_from_h5(h5_path, burn=0, thin=1):\n",
        "        b = HDFBackend(h5_path)\n",
        "        it = int(getattr(b, \"iteration\", b.get_chain().shape[0]))\n",
        "        burn_eff = int(min(int(burn), max(it - 1, 0)))\n",
        "        if it <= burn_eff:\n",
        "            raise RuntimeError(f\"H5 iteration={it} <= burn_eff={burn_eff} (post-burn empty): {os.path.basename(h5_path)}\")\n",
        "        chain = b.get_chain(discard=burn_eff, thin=int(thin), flat=False)\n",
        "        logp  = b.get_log_prob(discard=burn_eff, thin=int(thin), flat=False)\n",
        "        if chain.shape[0] < 1:\n",
        "            raise RuntimeError(f\"Empty post-burn chain in {os.path.basename(h5_path)}\")\n",
        "        ij = np.unravel_index(int(np.nanargmax(logp)), logp.shape)\n",
        "        theta = np.array(chain[ij[0], ij[1], :], dtype=float).ravel()\n",
        "        lp = float(logp[ij])\n",
        "        where = (int(burn_eff + ij[0]), int(ij[1]))\n",
        "        return theta, lp, where, it\n",
        "\n",
        "    buckets = _list_family_files(CAMPAIGN_DIR)\n",
        "    famL = _choose_family(buckets, \"LCDM\")\n",
        "    famE = _choose_family(buckets, \"ECTI\")\n",
        "    if famL is None or famE is None:\n",
        "        raise RuntimeError(f\"Families not found in CAMPAIGN_DIR. Found={sorted(buckets.keys())}\")\n",
        "\n",
        "    filesL = buckets[famL]\n",
        "    filesE = buckets[famE]\n",
        "    runidsL = [(_extract_runid(p), p) for p in filesL]\n",
        "    runidsE = [(_extract_runid(p), p) for p in filesE]\n",
        "    runidsL = [(rid, p) for rid, p in runidsL if rid is not None]\n",
        "    runidsE = [(rid, p) for rid, p in runidsE if rid is not None]\n",
        "    if not runidsL or not runidsE:\n",
        "        raise RuntimeError(\"Could not parse run IDs from H5 files.\")\n",
        "\n",
        "    common = sorted(set(r for r,_ in runidsL) & set(r for r,_ in runidsE))\n",
        "    if not common:\n",
        "        raise RuntimeError(\"No common run_id between LCDM and ECTI families.\")\n",
        "    rid_ref = int(max(common))\n",
        "\n",
        "    # Evaluate every common run (per-run summary)\n",
        "    rows = []\n",
        "    best = None\n",
        "\n",
        "    def _IC(chi2, k, n):\n",
        "        AIC = float(chi2 + 2*k)\n",
        "        BIC = float(chi2 + k*np.log(n))\n",
        "        return dict(AIC=AIC, BIC=BIC)\n",
        "\n",
        "    for rid in common:\n",
        "        pL = dict(runidsL).get(rid, None)\n",
        "        pE = dict(runidsE).get(rid, None)\n",
        "        if pL is None or pE is None:\n",
        "            continue\n",
        "\n",
        "        thL, lpL, whereL, itL = _map_from_h5(pL, burn=int(N_BURN), thin=int(globals().get(\"THIN\", 1)))\n",
        "        thE, lpE, whereE, itE = _map_from_h5(pE, burn=int(N_BURN), thin=int(globals().get(\"THIN\", 1)))\n",
        "\n",
        "        partsL, chi2L = chi2_breakdown_LCDM(thL)\n",
        "        partsE, chi2E = chi2_breakdown_ECTI(thE)\n",
        "\n",
        "        chi2L = float(chi2L)\n",
        "        chi2E = float(chi2E)\n",
        "        dchi2 = float(chi2E - chi2L)\n",
        "\n",
        "        IC_L = _IC(chi2L, K_PARAMS, N_DATA)\n",
        "        IC_E = _IC(chi2E, K_PARAMS, N_DATA)\n",
        "\n",
        "        rows.append(dict(\n",
        "            run_id=int(rid),\n",
        "            chi2_LCDM=chi2L,\n",
        "            chi2_ECTI=chi2E,\n",
        "            dchi2=dchi2,\n",
        "            dAIC=float(IC_E[\"AIC\"] - IC_L[\"AIC\"]),\n",
        "            dBIC=float(IC_E[\"BIC\"] - IC_L[\"BIC\"]),\n",
        "        ))\n",
        "\n",
        "        if rid == rid_ref:\n",
        "            best = dict(\n",
        "                policy=\"LAST_RUN\",\n",
        "                rid_ref=rid_ref,\n",
        "                LCDM=dict(h5=pL, theta_map=thL.tolist(), logp_map=float(lpL), where=whereL,\n",
        "                          parts=dict(partsL), chi2=chi2L, IC=IC_L),\n",
        "                ECTI=dict(h5=pE, theta_map=thE.tolist(), logp_map=float(lpE), where=whereE,\n",
        "                          parts=dict(partsE), chi2=chi2E, IC=IC_E),\n",
        "                meta=dict(\n",
        "                    CAMPAIGN_DIR=CAMPAIGN_DIR,\n",
        "                    families=dict(LCDM=famL, ECTI=famE),\n",
        "                    N_DATA=int(N_DATA),\n",
        "                    k_params=int(K_PARAMS),\n",
        "                    toggles=dict(\n",
        "                        USE_KIDS=bool(globals().get(\"USE_KIDS\", False)),\n",
        "                        USE_PLANCK_LENSING=bool(globals().get(\"USE_PLANCK_LENSING\", False)),\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "    if best is None:\n",
        "        raise RuntimeError(\"Failed to build BEST_13 for reference run.\")\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(\"run_id\").reset_index(drop=True)\n",
        "\n",
        "    globals()[\"FINAL_DF_10.1\"] = df\n",
        "    globals()[\"BEST_10.1\"] = best\n",
        "    globals()[\"N_DATA_10.1\"] = int(N_DATA)\n",
        "    globals()[\"K_PARAMS_10.1\"] = int(K_PARAMS)\n",
        "\n",
        "    # Save artifacts\n",
        "    csv_path = os.path.join(TAB_DIR, \"final_results_per_run_k5.csv\")\n",
        "    json_path = os.path.join(TAB_DIR, \"final_results_last_run_k5.json\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"10.1) FINAL RESULTS — FAIR k=5 (LAST RUN POLICY)\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"CAMPAIGN_DIR =\", CAMPAIGN_DIR)\n",
        "    print(\"Families     =\", f\"LCDM:{famL} | ECTI:{famE}\")\n",
        "    print(f\"N_data       = {N_DATA} (SN={N_SN}, BAO={N_BAO}, RSD={N_RSD}, KiDS={N_KIDS}, CMB={N_CMB}, LENS={N_LENS})\")\n",
        "    print(f\"k_params     = {K_PARAMS}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"Per-run summary:\")\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(f\"REFERENCE = LAST common run_id = run{rid_ref:03d}\")\n",
        "    print(\"LCDM: file=\", os.path.basename(best[\"LCDM\"][\"h5\"]), \"| chi2=\", f\"{best['LCDM']['chi2']:.3f}\",\n",
        "          \"| AIC=\", f\"{best['LCDM']['IC']['AIC']:.3f}\", \"| BIC=\", f\"{best['LCDM']['IC']['BIC']:.3f}\")\n",
        "    print(\"ECTI: file=\", os.path.basename(best[\"ECTI\"][\"h5\"]), \"| chi2=\", f\"{best['ECTI']['chi2']:.3f}\",\n",
        "          \"| AIC=\", f\"{best['ECTI']['IC']['AIC']:.3f}\", \"| BIC=\", f\"{best['ECTI']['IC']['BIC']:.3f}\")\n",
        "    print(\"Δχ²_ref (ECTI−LCDM) =\", f\"{(best['ECTI']['chi2']-best['LCDM']['chi2']):+.6f}\")\n",
        "    print(\"====================================================\")\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"✔ Saved:\", json_path)\n",
        "    print(\"✔ Stored: FINAL_DF_10.1, BEST_10.1, N_DATA_10.1, K_PARAMS_10.1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-a6XLg_-UF5"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.2) Run-to-run stability (FAIR k=5) — uses FINAL_DF_10.1\n",
        "#   - prints Δχ² / ΔAIC / ΔBIC stability across paired runs\n",
        "#   - saves CSV summary\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_cd = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "_SKIP_POST = (not _RUN_MCMC) or (not isinstance(_cd, str)) or (not os.path.isdir(_cd))\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 10.2 (stability): RUN_MCMC=False or CAMPAIGN_DIR missing/invalid.\")\n",
        "else:\n",
        "    if \"FINAL_DF_10.1\" not in globals():\n",
        "        print(\"⏭ Skipping Cell 10.2 (stability): FINAL_DF_13 not found (run Cell 10.1 first).\")\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "    # Rehydrate TAB_DIR (Run-All safe)\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(_cd, \"tables\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    df = globals()[\"FINAL_DF_10.1\"].copy()\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        raise RuntimeError(\"FINAL_DF_10.1 is missing or empty.\")\n",
        "\n",
        "    # Ensure expected columns exist\n",
        "    for col in [\"run_id\", \"chi2_LCDM\", \"chi2_ECTI\", \"dchi2\", \"dAIC\", \"dBIC\"]:\n",
        "        if col not in df.columns:\n",
        "            raise RuntimeError(f\"FINAL_DF_10.1 missing required column: {col}\")\n",
        "\n",
        "    # Summary stats\n",
        "    dchi = df[\"dchi2\"].astype(float).values\n",
        "    dAIC = df[\"dAIC\"].astype(float).values\n",
        "    dBIC = df[\"dBIC\"].astype(float).values\n",
        "\n",
        "    stats = dict(\n",
        "        dchi2_mean=float(np.mean(dchi)),\n",
        "        dchi2_std=float(np.std(dchi, ddof=1)) if dchi.size > 1 else float(\"nan\"),\n",
        "        dAIC_mean=float(np.mean(dAIC)),\n",
        "        dAIC_std=float(np.std(dAIC, ddof=1)) if dAIC.size > 1 else float(\"nan\"),\n",
        "        dBIC_mean=float(np.mean(dBIC)),\n",
        "        dBIC_std=float(np.std(dBIC, ddof=1)) if dBIC.size > 1 else float(\"nan\"),\n",
        "        n_runs=int(dchi.size),\n",
        "    )\n",
        "\n",
        "    out = df[[\"run_id\", \"chi2_LCDM\", \"chi2_ECTI\", \"dchi2\", \"dAIC\", \"dBIC\"]].copy()\n",
        "    csv_path = os.path.join(TAB_DIR_LOCAL, \"run_to_run_stability_k5.csv\")\n",
        "    out.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"10.2) Run-to-run stability (FAIR k=5)\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(f\"CAMPAIGN_DIR = {globals().get('CAMPAIGN_DIR')}\")\n",
        "    print(out.to_string(index=False))\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"Summary stats:\")\n",
        "    for k, v in stats.items():\n",
        "        print(f\"  {k:12s} = {v}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    globals()[\"STABILITY_STATS_10.2\"] = stats\n",
        "    globals()[\"STABILITY_DF_10.2\"] = out\n",
        "    print(\"✔ Stored: STABILITY_STATS_10.2, STABILITY_DF_10.2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6-lBonM-UF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.3) Combined posterior (p16/p50/p84) — FAIR k=5\n",
        "#   - Reads ALL H5 in CAMPAIGN_DIR for LCDM_k5 and ECTI_k5\n",
        "#   - Concatenates post-burn samples (flat)\n",
        "#   - Exports quantiles + combined samples to NPZ\n",
        "# ============================================================\n",
        "\n",
        "import os, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from emcee.backends import HDFBackend\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_cd = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "_SKIP_POST = (not _RUN_MCMC) or (not isinstance(_cd, str)) or (not os.path.isdir(_cd))\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 10.3 (combined posterior): RUN_MCMC=False or CAMPAIGN_DIR missing/invalid.\")\n",
        "else:\n",
        "    if not any(f.lower().endswith(\".h5\") for f in os.listdir(_cd)):\n",
        "        _SKIP_POST = True\n",
        "        print(\"⏭ Skipping Cell 10.3 (combined posterior): no .h5 files found in CAMPAIGN_DIR=\" + str(_cd))\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    # Required symbols\n",
        "    _required = [\"CAMPAIGN_DIR\", \"N_BURN\", \"THIN\"]\n",
        "    _missing = [k for k in _required if k not in globals()]\n",
        "    if _missing:\n",
        "        raise NameError(\"Missing required symbols before 10.3: \" + \", \".join(_missing))\n",
        "\n",
        "    # Output dirs under campaign (standard, repo-friendly)\n",
        "    TAB_DIR_LOCAL  = globals().get(\"TAB_DIR\",  os.path.join(_cd, \"tables\"))\n",
        "    POST_DIR_LOCAL = globals().get(\"POST_DIR\", os.path.join(_cd, \"posteriors\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(POST_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "    globals()[\"POST_DIR\"] = POST_DIR_LOCAL\n",
        "\n",
        "    PARAM_NAMES = [\"H0\", \"Om0\", \"sigma8\", \"M\", \"wb\"]\n",
        "\n",
        "    def _list_family_files(campaign_dir):\n",
        "        files = [f for f in os.listdir(campaign_dir) if f.lower().endswith(\".h5\")]\n",
        "        pat = re.compile(r\"^(.*)_run(\\d+)\\.h5$\", re.IGNORECASE)\n",
        "        buckets = {}\n",
        "        for f in files:\n",
        "            m = pat.match(f)\n",
        "            if not m:\n",
        "                continue\n",
        "            fam = m.group(1).upper()\n",
        "            rid = int(m.group(2))\n",
        "            if not (fam.startswith(\"LCDM\") or fam.startswith(\"ECTI\")):\n",
        "                continue\n",
        "            buckets.setdefault(fam, []).append((rid, os.path.join(campaign_dir, f)))\n",
        "        for fam in list(buckets.keys()):\n",
        "            buckets[fam] = [p for _, p in sorted(buckets[fam], key=lambda x: x[0])]\n",
        "        return buckets\n",
        "\n",
        "    def _choose_family(buckets, kind=\"LCDM\"):\n",
        "        keys = sorted([k for k in buckets.keys() if k.startswith(kind.upper())])\n",
        "        if not keys:\n",
        "            return None\n",
        "        pref = f\"{kind.upper()}_K5\"\n",
        "        for k in keys:\n",
        "            if pref in k:\n",
        "                return k\n",
        "        return keys[0]\n",
        "\n",
        "    def _read_flat_samples(h5_path, burn, thin):\n",
        "        b = HDFBackend(h5_path)\n",
        "        it = int(getattr(b, \"iteration\", b.get_chain().shape[0]))\n",
        "        if it <= 0:\n",
        "            raise RuntimeError(f\"H5 has no samples: {os.path.basename(h5_path)}\")\n",
        "\n",
        "        burn_eff = int(min(int(burn), max(it - 1, 0)))\n",
        "        if it <= burn_eff:\n",
        "            raise RuntimeError(\n",
        "                f\"H5 iteration={it} <= burn_eff={burn_eff} (post-burn empty): {os.path.basename(h5_path)}\"\n",
        "            )\n",
        "\n",
        "        flat = b.get_chain(discard=burn_eff, thin=int(thin), flat=True)\n",
        "        flat = np.asarray(flat, dtype=float)\n",
        "\n",
        "        if flat.ndim != 2 or flat.shape[1] != 5:\n",
        "            raise ValueError(\n",
        "                f\"Expected flat chain (Nsamp,5) in {os.path.basename(h5_path)}, got {flat.shape}\"\n",
        "            )\n",
        "        if flat.shape[0] < 1:\n",
        "            raise RuntimeError(f\"Empty flat chain post-burn in {os.path.basename(h5_path)}\")\n",
        "        return flat\n",
        "\n",
        "    def _quantiles(S):\n",
        "        q16 = np.percentile(S, 16, axis=0)\n",
        "        q50 = np.percentile(S, 50, axis=0)\n",
        "        q84 = np.percentile(S, 84, axis=0)\n",
        "        return q16, q50, q84\n",
        "\n",
        "    buckets = _list_family_files(CAMPAIGN_DIR)\n",
        "    famL = _choose_family(buckets, \"LCDM\")\n",
        "    famE = _choose_family(buckets, \"ECTI\")\n",
        "    if famL is None or famE is None:\n",
        "        raise RuntimeError(f\"Families not found in CAMPAIGN_DIR. Found={sorted(buckets.keys())}\")\n",
        "\n",
        "    filesL = buckets[famL]\n",
        "    filesE = buckets[famE]\n",
        "    if not filesL or not filesE:\n",
        "        raise RuntimeError(f\"Empty H5 family lists: LCDM={len(filesL)} ECTI={len(filesE)}\")\n",
        "\n",
        "    samplesL = [ _read_flat_samples(p, burn=int(N_BURN), thin=int(THIN)) for p in filesL ]\n",
        "    samplesE = [ _read_flat_samples(p, burn=int(N_BURN), thin=int(THIN)) for p in filesE ]\n",
        "\n",
        "    S_L = np.vstack(samplesL) if len(samplesL) else np.empty((0,5), float)\n",
        "    S_E = np.vstack(samplesE) if len(samplesE) else np.empty((0,5), float)\n",
        "\n",
        "    if S_L.shape[0] < 10 or S_E.shape[0] < 10:\n",
        "        raise RuntimeError(f\"Too few post-burn samples: LCDM={S_L.shape[0]}, ECTI={S_E.shape[0]}\")\n",
        "\n",
        "    qL16, qL50, qL84 = _quantiles(S_L)\n",
        "    qE16, qE50, qE84 = _quantiles(S_E)\n",
        "\n",
        "    dfq = pd.DataFrame({\n",
        "        \"param\": PARAM_NAMES,\n",
        "        \"LCDM_p16\": qL16, \"LCDM_p50\": qL50, \"LCDM_p84\": qL84,\n",
        "        \"ECTI_p16\": qE16, \"ECTI_p50\": qE50, \"ECTI_p84\": qE84,\n",
        "    })\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"10.3) Combined posterior quantiles (FAIR k=5)\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"CAMPAIGN_DIR =\", CAMPAIGN_DIR)\n",
        "    print(f\"LCDM family={famL} | files={len(filesL)} | samples={S_L.shape[0]}\")\n",
        "    print(f\"ECTI family={famE} | files={len(filesE)} | samples={S_E.shape[0]}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(dfq.to_string(index=False))\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    csv_path = os.path.join(TAB_DIR_LOCAL, \"posterior_quantiles_combined_k5.csv\")\n",
        "    dfq.to_csv(csv_path, index=False)\n",
        "\n",
        "    npz_path = os.path.join(POST_DIR_LOCAL, \"combined_posteriors_k5.npz\")\n",
        "    np.savez_compressed(\n",
        "        npz_path,\n",
        "        param_names=np.array(PARAM_NAMES, dtype=object),\n",
        "        samples_LCDM=S_L,\n",
        "        samples_ECTI=S_E,\n",
        "        q_LCDM=np.vstack([qL16, qL50, qL84]),\n",
        "        q_ECTI=np.vstack([qE16, qE50, qE84]),\n",
        "        family_LCDM=famL,\n",
        "        family_ECTI=famE,\n",
        "        CAMPAIGN_DIR=CAMPAIGN_DIR,\n",
        "        N_BURN=int(N_BURN),\n",
        "        THIN=int(THIN),\n",
        "    )\n",
        "\n",
        "    globals()[\"POST_SAMPLES_LCDM_10.3\"] = S_L\n",
        "    globals()[\"POST_SAMPLES_ECTI_10.3\"] = S_E\n",
        "    globals()[\"POST_QUANTILES_10.3\"] = dfq\n",
        "\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"✔ Saved:\", npz_path)\n",
        "    print(\"✔ Stored: POST_SAMPLES_*_10.3, POST_QUANTILES_10.3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KLEZbFIHqRZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.4) Ablations / robustness (subset χ²) @ BEST_10.1 reference MAP — FAIR k=5\n",
        "#   - NO refit, NO rerun MCMC: evaluates subset totals at the same reference MAP\n",
        "#   - Saves CSV in TAB_DIR\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_cd = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 10.4 (ablations): RUN_MCMC=False.\")\n",
        "else:\n",
        "    if \"BEST_10.1\" not in globals():\n",
        "        print(\"⏭ Skipping Cell 10.4 (ablations): BEST_10.1 not found (run Cell 10.1 first).\")\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    best = globals()[\"BEST_10.1\"]\n",
        "    partsL = dict(best[\"LCDM\"][\"parts\"])\n",
        "    partsE = dict(best[\"ECTI\"][\"parts\"])\n",
        "\n",
        "    # Output dir\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"tables\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    # Include LENS if present\n",
        "    ALL_KEYS = [\"SN\", \"BAO\", \"RSD\", \"KiDS\", \"CMB\", \"LENS\"]\n",
        "    present = [k for k in ALL_KEYS if (k in partsL) or (k in partsE)]\n",
        "\n",
        "    def _sum_parts(parts, keys):\n",
        "        return float(sum(float(parts.get(k, 0.0)) for k in keys))\n",
        "\n",
        "    ablations = [\n",
        "        (\"ALL\",                 present),\n",
        "        (\"NO_CMB\",              [k for k in present if k != \"CMB\"]),\n",
        "        (\"NO_LENS\",             [k for k in present if k != \"LENS\"]),\n",
        "        (\"NO_RSD\",              [k for k in present if k != \"RSD\"]),\n",
        "        (\"NO_KIDS\",             [k for k in present if k != \"KiDS\"]),\n",
        "        (\"BG_ONLY_SN+BAO+CMB\",  [k for k in [\"SN\", \"BAO\", \"CMB\"] if k in present]),\n",
        "    ]\n",
        "\n",
        "    rows = []\n",
        "    for name, keys in ablations:\n",
        "        chi2L = _sum_parts(partsL, keys)\n",
        "        chi2E = _sum_parts(partsE, keys)\n",
        "        rows.append({\n",
        "            \"ablation\": name,\n",
        "            \"keys\": \"+\".join(keys),\n",
        "            \"chi2_LCDM_subset\": chi2L,\n",
        "            \"chi2_ECTI_subset\": chi2E,\n",
        "            \"dchi2_subset (E-L)\": float(chi2E - chi2L),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    chi2L_all = float(df.loc[df[\"ablation\"]==\"ALL\",\"chi2_LCDM_subset\"].iloc[0])\n",
        "    chi2E_all = float(df.loc[df[\"ablation\"]==\"ALL\",\"chi2_ECTI_subset\"].iloc[0])\n",
        "    df[\"dchi2_LCDM_vs_ALL\"] = df[\"chi2_LCDM_subset\"] - chi2L_all\n",
        "    df[\"dchi2_ECTI_vs_ALL\"] = df[\"chi2_ECTI_subset\"] - chi2E_all\n",
        "\n",
        "    print(\"====================================================\")\n",
        "    print(\"10.4) Ablations @ BEST_10.1 reference MAP (NO refit)\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"Probes present:\", present)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"NOTE: subset totals evaluated at SAME reference MAP (no re-optimization).\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    csv_path = os.path.join(TAB_DIR_LOCAL, \"ablations_subset_chi2_at_reference_MAP_k5.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    globals()[\"ABLATIONS_DF_10.4\"] = df\n",
        "\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"✔ Stored: ABLATIONS_DF_10.4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwpoyGqcHqRZ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10.5) Summary figure — Δχ² per probe (ECTI - LCDM) @ BEST_10.1 reference MAP\n",
        "#   - One \"wow\" figure that summarizes where the model wins/loses\n",
        "#   - Saves PNG + CSV\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_cd = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 10.5 (Δχ² per probe fig): RUN_MCMC=False.\")\n",
        "else:\n",
        "    if \"BEST_10.1\" not in globals():\n",
        "        print(\"⏭ Skipping Cell 10.5: BEST_10.1 not found (run Cell 10.1 first).\")\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    best = globals()[\"BEST_10.1\"]\n",
        "    partsL = dict(best[\"LCDM\"][\"parts\"])\n",
        "    partsE = dict(best[\"ECTI\"][\"parts\"])\n",
        "\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    FIG_DIR_LOCAL = globals().get(\"FIG_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"figures\"))\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"tables\"))\n",
        "    os.makedirs(FIG_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"FIG_DIR\"] = FIG_DIR_LOCAL\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    # include LENS if present\n",
        "    order = [\"SN\", \"BAO\", \"RSD\", \"KiDS\", \"CMB\", \"LENS\"]\n",
        "    keys = [k for k in order if (k in partsL) or (k in partsE)]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"probe\": keys,\n",
        "        \"chi2_LCDM\": [float(partsL.get(k, 0.0)) for k in keys],\n",
        "        \"chi2_ECTI\": [float(partsE.get(k, 0.0)) for k in keys],\n",
        "    })\n",
        "    df[\"dchi2 (ECTI-LCDM)\"] = df[\"chi2_ECTI\"] - df[\"chi2_LCDM\"]\n",
        "    df[\"abs\"] = np.abs(df[\"dchi2 (ECTI-LCDM)\"])\n",
        "    df_sorted = df.sort_values(\"abs\", ascending=False).drop(columns=[\"abs\"]).reset_index(drop=True)\n",
        "\n",
        "    csv_path = os.path.join(TAB_DIR_LOCAL, \"delta_chi2_per_probe_k5.csv\")\n",
        "    df_sorted.to_csv(csv_path, index=False)\n",
        "\n",
        "    fig_path = os.path.join(FIG_DIR_LOCAL, \"delta_chi2_per_probe_k5.png\")\n",
        "    plt.figure(figsize=(7.8, 4.6))\n",
        "    x = np.arange(len(df_sorted))\n",
        "    vals = df_sorted[\"dchi2 (ECTI-LCDM)\"].values\n",
        "    plt.bar(x, vals)\n",
        "    plt.axhline(0.0)\n",
        "    plt.xticks(x, df_sorted[\"probe\"].values)\n",
        "    plt.ylabel(r\"$\\Delta \\chi^2$ (ECTI $-$ $\\Lambda$CDM)\")\n",
        "    plt.title(r\"Where ECTI wins/loses (reference MAP, FAIR $k=5$)\")\n",
        "\n",
        "    for i, v in enumerate(vals):\n",
        "        plt.text(i, v, f\"{v:+.2f}\", ha=\"center\", va=(\"bottom\" if v >= 0 else \"top\"))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "    globals()[\"DCHI2_PER_PROBE_DF_10.5\"] = df_sorted\n",
        "\n",
        "    print(\"✔ Saved:\", csv_path)\n",
        "    print(\"✔ Saved:\", fig_path)\n",
        "    print(\"✔ Stored: DCHI2_PER_PROBE_DF_10.5\")\n",
        "    print(df_sorted.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD7IGbufncck"
      },
      "source": [
        "# 11) Publication-ready tables (LaTeX)\n",
        "\n",
        "This section generates **LaTeX tables** summarizing the final results\n",
        "for direct inclusion in an article, report, or arXiv submission.\n",
        "\n",
        "Two tables are produced:\n",
        "\n",
        "1. Best-fit χ² breakdown by dataset (ΛCDM vs ECTI-P)\n",
        "2. Model comparison using χ², AIC, and BIC\n",
        "\n",
        "The output can be copied directly into a LaTeX document\n",
        "without any modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjqs4YbRzRln"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11.1) LaTeX tables for publication — FAIR k=5 (BEST_10.1 reference MAP)\n",
        "#   - χ² breakdown table (includes LENS if present)\n",
        "#   - IC (AIC/BIC) table (from BEST_10.1)\n",
        "#   - Saves .tex into TAB_DIR\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_cd = globals().get(\"CAMPAIGN_DIR\", None)\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 11.1 (LaTeX tables): RUN_MCMC=False.\")\n",
        "else:\n",
        "    if \"BEST_10.1\" not in globals():\n",
        "        print(\"⏭ Skipping Cell 11.1: BEST_10.1 not found (run Cell 10.1 first).\")\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    best = globals()[\"BEST_10.1\"]\n",
        "\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"tables\"))\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    partsL = dict(best[\"LCDM\"][\"parts\"])\n",
        "    partsE = dict(best[\"ECTI\"][\"parts\"])\n",
        "\n",
        "    IC_L = dict(best[\"LCDM\"].get(\"IC\", {}))\n",
        "    IC_E = dict(best[\"ECTI\"].get(\"IC\", {}))\n",
        "\n",
        "    def latex_float(x, ndigits=3):\n",
        "        return f\"{float(x):.{ndigits}f}\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # Table 1: χ² breakdown\n",
        "    # -----------------------------\n",
        "    order = [\"SN\", \"BAO\", \"RSD\", \"KiDS\", \"CMB\", \"LENS\"]\n",
        "    keys = [k for k in order if (k in partsL) or (k in partsE)]\n",
        "    chi2L_tot = float(best[\"LCDM\"][\"chi2\"])\n",
        "    chi2E_tot = float(best[\"ECTI\"][\"chi2\"])\n",
        "\n",
        "    rows = []\n",
        "    for k in keys:\n",
        "        rows.append((k, partsL.get(k, 0.0), partsE.get(k, 0.0)))\n",
        "\n",
        "    latex_chi2 = []\n",
        "    latex_chi2.append(r\"\\begin{table}[h]\")\n",
        "    latex_chi2.append(r\"\\centering\")\n",
        "    latex_chi2.append(r\"\\caption{Best-fit $\\chi^2$ contributions by dataset (reference MAP, FAIR $k=5$).}\")\n",
        "    latex_chi2.append(r\"\\begin{tabular}{lcc}\")\n",
        "    latex_chi2.append(r\"\\hline\")\n",
        "    latex_chi2.append(r\"Dataset & $\\Lambda$CDM & ECTI-P \\\\\")\n",
        "    latex_chi2.append(r\"\\hline\")\n",
        "    for k, vL, vE in rows:\n",
        "        latex_chi2.append(f\"{k} & {latex_float(vL)} & {latex_float(vE)} \\\\\\\\\")\n",
        "    latex_chi2.append(r\"\\hline\")\n",
        "    latex_chi2.append(f\"Total & {latex_float(chi2L_tot)} & {latex_float(chi2E_tot)} \\\\\\\\\")\n",
        "    latex_chi2.append(r\"\\hline\")\n",
        "    latex_chi2.append(r\"\\end{tabular}\")\n",
        "    latex_chi2.append(r\"\\end{table}\")\n",
        "    latex_chi2_str = \"\\n\".join(latex_chi2)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Table 2: Information criteria\n",
        "    # -----------------------------\n",
        "    # Note: AIC/BIC computed in Cell 10.1; here we only format/report.\n",
        "    aicL = IC_L.get(\"AIC\", np.nan)\n",
        "    bicL = IC_L.get(\"BIC\", np.nan)\n",
        "    aicE = IC_E.get(\"AIC\", np.nan)\n",
        "    bicE = IC_E.get(\"BIC\", np.nan)\n",
        "\n",
        "    latex_ic = []\n",
        "    latex_ic.append(r\"\\begin{table}[h]\")\n",
        "    latex_ic.append(r\"\\centering\")\n",
        "    latex_ic.append(r\"\\caption{Information criteria at the reference MAP (FAIR $k=5$).}\")\n",
        "    latex_ic.append(r\"\\begin{tabular}{lcc}\")\n",
        "    latex_ic.append(r\"\\hline\")\n",
        "    latex_ic.append(r\"Metric & $\\Lambda$CDM & ECTI-P \\\\\")\n",
        "    latex_ic.append(r\"\\hline\")\n",
        "    latex_ic.append(f\"AIC & {latex_float(aicL)} & {latex_float(aicE)} \\\\\\\\\")\n",
        "    latex_ic.append(f\"BIC & {latex_float(bicL)} & {latex_float(bicE)} \\\\\\\\\")\n",
        "    latex_ic.append(r\"\\hline\")\n",
        "    latex_ic.append(r\"\\end{tabular}\")\n",
        "    latex_ic.append(r\"\\end{table}\")\n",
        "    latex_ic_str = \"\\n\".join(latex_ic)\n",
        "\n",
        "    out1 = os.path.join(TAB_DIR_LOCAL, \"table_chi2_breakdown_k5.tex\")\n",
        "    out2 = os.path.join(TAB_DIR_LOCAL, \"table_information_criteria_k5.tex\")\n",
        "    with open(out1, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(latex_chi2_str + \"\\n\")\n",
        "    with open(out2, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(latex_ic_str + \"\\n\")\n",
        "\n",
        "    print(\"✔ Saved:\", out1)\n",
        "    print(\"✔ Saved:\", out2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L4PoBq1n9yf"
      },
      "source": [
        "# 12) Posterior distributions (corner plots)\n",
        "\n",
        "This section produces **corner plots** of the posterior distributions\n",
        "for both ΛCDM and ECTI-P using the converged MCMC chains.\n",
        "\n",
        "These plots are intended for:\n",
        "- visual inspection of parameter degeneracies,\n",
        "- supplementary material (paper / arXiv),\n",
        "- sanity checks against pathological posteriors.\n",
        "\n",
        "The plots are generated **only if MCMC chains are already available**.\n",
        "No MCMC run is triggered in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wrXXQ_px03c"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 12.1) Corner plot — FAIR k=5 (combined posterior if available)\n",
        "#   - Prefers combined samples from Cell 10.3 (POST_SAMPLES_*_10.3)\n",
        "#   - Falls back to LAST run H5 (BEST_10.1) if combined samples missing\n",
        "#   - Saves PNG into FIG_DIR\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 12.1 (corner): RUN_MCMC=False.\")\n",
        "else:\n",
        "    # Need either combined samples OR BEST_10.1 to fallback\n",
        "    if (\"POST_SAMPLES_LCDM_10.3\" not in globals()) and (\"BEST_10.1\" not in globals()):\n",
        "        print(\"⏭ Skipping Cell 12.1 (corner): need POST_SAMPLES_*_10.3 or BEST_10.1 (run Cells 10.1/10.3 first).\")\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    FIG_DIR_LOCAL = globals().get(\"FIG_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"figures\"))\n",
        "    os.makedirs(FIG_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"FIG_DIR\"] = FIG_DIR_LOCAL\n",
        "\n",
        "    # Optional dependency: corner\n",
        "    try:\n",
        "        import corner\n",
        "        import matplotlib.lines as mlines\n",
        "        _has_corner = True\n",
        "    except Exception:\n",
        "        _has_corner = False\n",
        "\n",
        "    PARAM_NAMES = [\"H0\", \"Om0\", \"sigma8\", \"M\", \"wb\"]\n",
        "\n",
        "    def _load_from_best_10_1(model_key: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Fallback loader: reads LAST-run H5 paths stored in globals()['BEST_10.1'].\n",
        "        Returns a (Nsamp, 5) flat chain after discard/thin.\n",
        "        \"\"\"\n",
        "        from emcee.backends import HDFBackend\n",
        "\n",
        "        if \"BEST_10.1\" not in globals():\n",
        "            raise NameError(\"BEST_10.1 not found in globals() (run Cell 10.1 first).\")\n",
        "\n",
        "        best = globals()[\"BEST_10.1\"]\n",
        "        if model_key not in best or \"h5\" not in best[model_key]:\n",
        "            raise KeyError(f\"BEST_10.1 missing key '{model_key}[\\\"h5\\\"]'.\")\n",
        "\n",
        "        h5 = best[model_key][\"h5\"]\n",
        "        b = HDFBackend(h5)\n",
        "\n",
        "        it = int(getattr(b, \"iteration\", b.get_chain().shape[0]))\n",
        "        burn = int(globals().get(\"N_BURN\", 0))\n",
        "        thin = int(globals().get(\"THIN\", 1))\n",
        "        burn_eff = int(min(burn, max(it - 1, 0)))\n",
        "\n",
        "        if it <= burn_eff:\n",
        "            raise RuntimeError(f\"H5 iteration={it} <= burn_eff={burn_eff}: {os.path.basename(h5)}\")\n",
        "\n",
        "        flat = b.get_chain(discard=burn_eff, thin=thin, flat=True)\n",
        "        flat = np.asarray(flat, float)\n",
        "\n",
        "        if flat.ndim != 2 or flat.shape[1] != 5:\n",
        "            raise RuntimeError(f\"Expected (Nsamp,5) chain in {os.path.basename(h5)}, got {flat.shape}\")\n",
        "\n",
        "        return flat\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load samples\n",
        "    # -----------------------------\n",
        "    if (\"POST_SAMPLES_LCDM_10.3\" in globals()) and (\"POST_SAMPLES_ECTI_10.3\" in globals()):\n",
        "        S_L = np.asarray(globals()[\"POST_SAMPLES_LCDM_10.3\"], float)\n",
        "        S_E = np.asarray(globals()[\"POST_SAMPLES_ECTI_10.3\"], float)\n",
        "        source = \"combined posterior (10.3)\"\n",
        "    else:\n",
        "        S_L = _load_from_best_10_1(\"LCDM\")\n",
        "        S_E = _load_from_best_10_1(\"ECTI\")\n",
        "        source = \"LAST run H5 (BEST_10.1)\"\n",
        "\n",
        "    if S_L.ndim != 2 or S_L.shape[1] != 5:\n",
        "        raise RuntimeError(f\"Expected LCDM samples shape (Nsamp,5), got {S_L.shape}\")\n",
        "    if S_E.ndim != 2 or S_E.shape[1] != 5:\n",
        "        raise RuntimeError(f\"Expected ECTI samples shape (Nsamp,5), got {S_E.shape}\")\n",
        "\n",
        "    if S_L.shape[0] < 100 or S_E.shape[0] < 100:\n",
        "        raise RuntimeError(f\"Too few samples for corner: LCDM={S_L.shape[0]}, ECTI={S_E.shape[0]}\")\n",
        "\n",
        "    out_png = os.path.join(FIG_DIR_LOCAL, \"corner_LCDM_vs_ECTI_k5.png\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Plot\n",
        "    # -----------------------------\n",
        "    if not _has_corner:\n",
        "        print(\"⚠ 'corner' not installed. Skipping corner plot generation.\")\n",
        "    else:\n",
        "        # Requested visual distinction: ΛCDM = blue, ECTI = red\n",
        "        fig = corner.corner(\n",
        "            S_L,\n",
        "            labels=PARAM_NAMES,\n",
        "            show_titles=True,\n",
        "            title_fmt=\".3f\",\n",
        "            color=\"C0\",\n",
        "            plot_datapoints=False,\n",
        "            fill_contours=False,\n",
        "        )\n",
        "        corner.corner(\n",
        "            S_E,\n",
        "            labels=PARAM_NAMES,\n",
        "            show_titles=True,\n",
        "            title_fmt=\".3f\",\n",
        "            fig=fig,\n",
        "            color=\"C3\",\n",
        "            plot_datapoints=False,\n",
        "            fill_contours=False,\n",
        "        )\n",
        "\n",
        "        # Legend (top-right)\n",
        "        h_lcdm = mlines.Line2D([], [], color=\"C0\", label=\"ΛCDM\")\n",
        "        h_ecti = mlines.Line2D([], [], color=\"C3\", label=\"ECTI-P\")\n",
        "        fig.legend(handles=[h_lcdm, h_ecti], loc=\"upper right\", frameon=False)\n",
        "\n",
        "        fig.savefig(out_png, dpi=180)\n",
        "        plt.close(fig)\n",
        "        print(\"✔ Saved:\", out_png)\n",
        "        print(\"Source:\", source)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwADu-Nopgh-"
      },
      "source": [
        "# 13) Residual plots: SN Ia, BAO, and RSD\n",
        "\n",
        "In this section, we compare the performance of ΛCDM and ECTI-P\n",
        "directly at the level of observables by inspecting residuals.\n",
        "\n",
        "Residuals are defined as:\n",
        "- Supernovae:     Δμ(z) = μ_obs − μ_th\n",
        "- BAO:            Δ(D_V / r_s)\n",
        "- RSD:            Δ(fσ₈)\n",
        "\n",
        "These plots provide an intuitive, model-independent view of\n",
        "where each model succeeds or fails relative to the data.\n",
        "\n",
        "All residuals are computed using the best-fit parameters\n",
        "obtained from the converged MCMC runs (R̂-validated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3dHZTu4RD_V"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 13.1) Residuals / sanity plots @ BEST_10.1 reference MAP — FAIR k=5 (LAST run)\n",
        "#   - SN residuals: (mu_obs - mu_model)/sigma\n",
        "#   - BAO pulls: (obs - theory)/sigma (diag from cov)\n",
        "#   - RSD pulls: (obs - pred)/err\n",
        "#   - Saves figures + CSV\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 13.1 (residuals): RUN_MCMC=False.\")\n",
        "else:\n",
        "    # need BEST_10.1 + core data arrays + model eval fns\n",
        "    required = [\n",
        "        \"BEST_10.1\",\n",
        "        \"z_SN\", \"mu_SN\", \"Cinv_SN\",\n",
        "        \"BAO_obs\", \"BAO_inv_cov\",\n",
        "        \"z_RSD\", \"fs8_obs\", \"fs8_err\",\n",
        "        \"mu_LCDM\", \"mu_ECTI\",\n",
        "        \"bao_theory_vector_LCDM\", \"bao_theory_vector_ECTI\",\n",
        "        \"fsigma8_theory\",\n",
        "    ]\n",
        "    missing = [k for k in required if k not in globals()]\n",
        "    if missing:\n",
        "        print(\"⏭ Skipping Cell 13.1 (residuals): missing required symbols:\", missing)\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    best = globals()[\"BEST_10.1\"]\n",
        "    thetaL = np.array(best[\"LCDM\"][\"theta_map\"], dtype=float).ravel()\n",
        "    thetaE = np.array(best[\"ECTI\"][\"theta_map\"], dtype=float).ravel()\n",
        "\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    FIG_DIR_LOCAL = globals().get(\"FIG_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"figures\"))\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"tables\"))\n",
        "    os.makedirs(FIG_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    globals()[\"FIG_DIR\"] = FIG_DIR_LOCAL\n",
        "    globals()[\"TAB_DIR\"] = TAB_DIR_LOCAL\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # SN residuals\n",
        "    # ------------------------------------------------------------\n",
        "    zsn = np.asarray(globals()[\"z_SN\"], float).ravel()\n",
        "    mu_obs = np.asarray(globals()[\"mu_SN\"], float).ravel()\n",
        "\n",
        "    muL = np.asarray(mu_LCDM(zsn, thetaL), float).ravel()\n",
        "    muE = np.asarray(mu_ECTI(zsn, thetaE), float).ravel()\n",
        "\n",
        "    Cinv = np.asarray(globals()[\"Cinv_SN\"], float)\n",
        "    C = np.linalg.inv(Cinv)\n",
        "    sig_sn = np.sqrt(np.diag(C))\n",
        "\n",
        "    resL = (mu_obs - muL) / sig_sn\n",
        "    resE = (mu_obs - muE) / sig_sn\n",
        "\n",
        "    df_sn = pd.DataFrame({\"z\": zsn, \"res_LCDM\": resL, \"res_ECTI\": resE, \"sigma_mu\": sig_sn})\n",
        "    df_sn.to_csv(os.path.join(TAB_DIR_LOCAL, \"residuals_SN.csv\"), index=False)\n",
        "\n",
        "    plt.figure(figsize=(7, 3.0))\n",
        "    plt.axhline(0.0, lw=0.8)\n",
        "    plt.scatter(zsn, resL, s=8, label=r\"$\\Lambda$CDM\")\n",
        "    plt.scatter(zsn, resE, s=8, label=\"ECTI-P\")\n",
        "    plt.xlabel(r\"$z$\")\n",
        "    plt.ylabel(r\"$(\\mu_{\\rm obs}-\\mu_{\\rm model})/\\sigma$\")\n",
        "    plt.title(\"SN residuals (Pantheon+)\")\n",
        "    plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR_LOCAL, \"residuals_SN.png\"), dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # BAO pulls\n",
        "    # ------------------------------------------------------------\n",
        "    ybao = np.asarray(globals()[\"BAO_obs\"], float).ravel()\n",
        "\n",
        "    tL = np.asarray(bao_theory_vector_LCDM(thetaL), float).ravel()\n",
        "    tE = np.asarray(bao_theory_vector_ECTI(thetaE), float).ravel()\n",
        "\n",
        "    if tL.size != ybao.size or tE.size != ybao.size:\n",
        "        raise RuntimeError(f\"BAO theory size mismatch: obs={ybao.size} LCDM={tL.size} ECTI={tE.size}\")\n",
        "\n",
        "    invcov_bao = np.asarray(globals()[\"BAO_inv_cov\"], float)\n",
        "    cov_bao = np.linalg.inv(invcov_bao)\n",
        "    sig_bao = np.sqrt(np.diag(cov_bao))\n",
        "\n",
        "    pullL_bao = (ybao - tL) / sig_bao\n",
        "    pullE_bao = (ybao - tE) / sig_bao\n",
        "\n",
        "    df_bao = pd.DataFrame({\n",
        "        \"i\": np.arange(len(ybao)),\n",
        "        \"pull_LCDM\": pullL_bao,\n",
        "        \"pull_ECTI\": pullE_bao,\n",
        "        \"sigma\": sig_bao,\n",
        "        \"obs\": ybao,\n",
        "        \"th_LCDM\": tL,\n",
        "        \"th_ECTI\": tE,\n",
        "    })\n",
        "    df_bao.to_csv(os.path.join(TAB_DIR_LOCAL, \"pulls_BAO.csv\"), index=False)\n",
        "\n",
        "    plt.figure(figsize=(7, 3.0))\n",
        "    plt.axhline(0.0, lw=0.8)\n",
        "    x = np.arange(len(ybao))\n",
        "    plt.plot(x, pullL_bao, marker=\"o\", label=r\"$\\Lambda$CDM\")\n",
        "    plt.plot(x, pullE_bao, marker=\"o\", label=\"ECTI-P\")\n",
        "    plt.xlabel(\"BAO point index\")\n",
        "    plt.ylabel(r\"$(\\mathrm{obs}-\\mathrm{th})/\\sigma$\")\n",
        "    plt.title(\"BAO pulls (DESI 2024)\")\n",
        "    plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR_LOCAL, \"pulls_BAO.png\"), dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # RSD pulls (fσ8)\n",
        "    # ------------------------------------------------------------\n",
        "    zrsd = np.asarray(globals()[\"z_RSD\"], float).ravel()\n",
        "    obs_rsd = np.asarray(globals()[\"fs8_obs\"], float).ravel()\n",
        "    err_rsd = np.asarray(globals()[\"fs8_err\"], float).ravel()\n",
        "\n",
        "    # fsigma8_theory signature observed in your notebook: fsigma8_theory(theta, model='LCDM')\n",
        "    predL = np.asarray(fsigma8_theory(thetaL, model=\"LCDM\"), float).ravel()\n",
        "    predE = np.asarray(fsigma8_theory(thetaE, model=\"ECTI\"), float).ravel()\n",
        "\n",
        "    if predL.size != obs_rsd.size or predE.size != obs_rsd.size:\n",
        "        raise RuntimeError(f\"RSD size mismatch: obs={obs_rsd.size} LCDM={predL.size} ECTI={predE.size}\")\n",
        "\n",
        "    pullL_rsd = (obs_rsd - predL) / err_rsd\n",
        "    pullE_rsd = (obs_rsd - predE) / err_rsd\n",
        "\n",
        "    df_rsd = pd.DataFrame({\n",
        "        \"z\": zrsd,\n",
        "        \"pull_LCDM\": pullL_rsd,\n",
        "        \"pull_ECTI\": pullE_rsd,\n",
        "        \"obs\": obs_rsd,\n",
        "        \"err\": err_rsd,\n",
        "        \"th_LCDM\": predL,\n",
        "        \"th_ECTI\": predE,\n",
        "    })\n",
        "    df_rsd.to_csv(os.path.join(TAB_DIR_LOCAL, \"pulls_RSD.csv\"), index=False)\n",
        "\n",
        "    plt.figure(figsize=(7, 3.0))\n",
        "    plt.axhline(0.0, lw=0.8)\n",
        "    plt.plot(zrsd, pullL_rsd, marker=\"o\", label=r\"$\\Lambda$CDM\")\n",
        "    plt.plot(zrsd, pullE_rsd, marker=\"o\", label=\"ECTI-P\")\n",
        "    plt.xlabel(r\"$z$\")\n",
        "    plt.ylabel(r\"$(f\\sigma_8^{\\rm obs}-f\\sigma_8^{\\rm th})/\\sigma$\")\n",
        "    plt.title(\"RSD pulls\")\n",
        "    plt.legend(frameon=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR_LOCAL, \"pulls_RSD.png\"), dpi=180)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"=== Cell 13.1 done ===\")\n",
        "    print(\"Saved figures to:\", FIG_DIR_LOCAL)\n",
        "    print(\"Saved CSV to:\", TAB_DIR_LOCAL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kEIgK_yj7AU"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 13.2) Referee checklist — reproducibility & coherence (FAIR k=5, LAST run)\n",
        "#   - Hard checks (skip-safe when RUN_MCMC=False)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Run-All guard (skip-safe)\n",
        "# -----------------------------\n",
        "_RUN_MCMC = bool(globals().get(\"RUN_MCMC\", True))\n",
        "_SKIP_POST = (not _RUN_MCMC)\n",
        "\n",
        "if _SKIP_POST:\n",
        "    print(\"⏭ Skipping Cell 13.2 (referee checklist): RUN_MCMC=False.\")\n",
        "else:\n",
        "    _required = [\n",
        "        \"BEST_10.1\",\n",
        "        \"CMB_OBS\", \"CMB_INV_COV\",\n",
        "        \"chi2_breakdown_LCDM\", \"chi2_breakdown_ECTI\",\n",
        "    ]\n",
        "    _missing = [k for k in _required if k not in globals()]\n",
        "    if _missing:\n",
        "        print(\"⏭ Skipping Cell 13.2 (referee checklist): missing required symbols:\", _missing)\n",
        "        _SKIP_POST = True\n",
        "\n",
        "if not _SKIP_POST:\n",
        "\n",
        "    best = globals()[\"BEST_10.1\"]\n",
        "    thL = np.asarray(best[\"LCDM\"][\"theta_map\"], float).ravel()\n",
        "    thE = np.asarray(best[\"ECTI\"][\"theta_map\"], float).ravel()\n",
        "\n",
        "    def _assert(cond, msg):\n",
        "        if not bool(cond):\n",
        "            raise AssertionError(msg)\n",
        "\n",
        "    # 0) Policy label\n",
        "    policy = str(best.get(\"policy\", \"UNKNOWN\"))\n",
        "    rid_ref = best.get(\"rid_ref\", None)\n",
        "\n",
        "    # 1) FAIR theta sizes\n",
        "    _assert(thL.size == 5, f\"LCDM theta at reference MAP must be θ5, got size {thL.size}\")\n",
        "    _assert(thE.size == 5, f\"ECTI theta at reference MAP must be θ5, got size {thE.size}\")\n",
        "\n",
        "    # 2) Planck lite 3D shapes\n",
        "    OBS = np.asarray(globals()[\"CMB_OBS\"], float).ravel()\n",
        "    INV = np.asarray(globals()[\"CMB_INV_COV\"], float)\n",
        "    _assert(OBS.size == 3, f\"CMB_OBS must have length 3 (R,lA,wb), got {OBS.size}\")\n",
        "    _assert(INV.shape == (3,3), f\"CMB_INV_COV must be 3x3, got {INV.shape}\")\n",
        "\n",
        "    if \"CMB_COV\" in globals():\n",
        "        COV = np.asarray(globals()[\"CMB_COV\"], float)\n",
        "        _assert(COV.shape == (3,3), f\"CMB_COV must be 3x3, got {COV.shape}\")\n",
        "    else:\n",
        "        try:\n",
        "            COV = np.linalg.inv(INV)\n",
        "        except Exception:\n",
        "            COV = np.linalg.pinv(INV)\n",
        "        _assert(COV.shape == (3,3), f\"Reconstructed CMB_COV must be 3x3, got {COV.shape}\")\n",
        "\n",
        "    def _max_abs(A):\n",
        "        A = np.asarray(A, float)\n",
        "        return float(np.max(np.abs(A))) if A.size else 0.0\n",
        "\n",
        "    sym_INV = _max_abs(INV - INV.T)\n",
        "    sym_COV = _max_abs(COV - COV.T)\n",
        "    _assert(sym_INV <= 1e-12 * (1.0 + _max_abs(INV)), f\"CMB_INV_COV not symmetric enough: {sym_INV:.3e}\")\n",
        "    _assert(sym_COV <= 1e-12 * (1.0 + _max_abs(COV)), f\"CMB_COV not symmetric enough: {sym_COV:.3e}\")\n",
        "\n",
        "    I = np.eye(3)\n",
        "    prod = COV @ INV\n",
        "    errI = _max_abs(prod - I)\n",
        "    _assert(errI <= 1e-8 * (1.0 + _max_abs(I)), f\"COV @ INV not close to I: {errI:.3e}\")\n",
        "\n",
        "    # 3) chi2 coherence\n",
        "    partsL, sumL = globals()[\"chi2_breakdown_LCDM\"](thL)\n",
        "    partsE, sumE = globals()[\"chi2_breakdown_ECTI\"](thE)\n",
        "    partsL = dict(partsL)\n",
        "    partsE = dict(partsE)\n",
        "\n",
        "    if \"chi2_tot_LCDM\" in globals() and callable(globals()[\"chi2_tot_LCDM\"]):\n",
        "        totL = float(globals()[\"chi2_tot_LCDM\"](thL))\n",
        "        _assert(abs(totL - float(sumL)) <= 1e-6*(1.0+abs(float(sumL))),\n",
        "                f\"LCDM: chi2_tot != sum(parts): {totL} vs {float(sumL)}\")\n",
        "    else:\n",
        "        totL = float(sumL)\n",
        "\n",
        "    if \"chi2_tot_ECTI\" in globals() and callable(globals()[\"chi2_tot_ECTI\"]):\n",
        "        totE = float(globals()[\"chi2_tot_ECTI\"](thE))\n",
        "        _assert(abs(totE - float(sumE)) <= 1e-6*(1.0+abs(float(sumE))),\n",
        "                f\"ECTI: chi2_tot != sum(parts): {totE} vs {float(sumE)}\")\n",
        "    else:\n",
        "        totE = float(sumE)\n",
        "\n",
        "    _assert(abs(float(sum(partsL.values())) - float(sumL)) <= 1e-9*(1.0+abs(float(sumL))),\n",
        "            \"LCDM: sum(parts dict) != reported total from chi2_breakdown_LCDM\")\n",
        "    _assert(abs(float(sum(partsE.values())) - float(sumE)) <= 1e-9*(1.0+abs(float(sumE))),\n",
        "            \"ECTI: sum(parts dict) != reported total from chi2_breakdown_ECTI\")\n",
        "\n",
        "    # 4) Output dirs exist\n",
        "    CAMPAIGN_DIR_LOCAL = globals().get(\"CAMPAIGN_DIR\", os.getcwd())\n",
        "    FIG_DIR_LOCAL = globals().get(\"FIG_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"figures\"))\n",
        "    TAB_DIR_LOCAL = globals().get(\"TAB_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"tables\"))\n",
        "    POST_DIR_LOCAL = globals().get(\"POST_DIR\", os.path.join(CAMPAIGN_DIR_LOCAL, \"posteriors\"))\n",
        "    os.makedirs(FIG_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(TAB_DIR_LOCAL, exist_ok=True)\n",
        "    os.makedirs(POST_DIR_LOCAL, exist_ok=True)\n",
        "\n",
        "    _assert(os.path.isdir(FIG_DIR_LOCAL), \"FIG_DIR missing (could not create).\")\n",
        "    _assert(os.path.isdir(TAB_DIR_LOCAL), \"TAB_DIR missing (could not create).\")\n",
        "    _assert(os.path.isdir(POST_DIR_LOCAL), \"POST_DIR missing (could not create).\")\n",
        "\n",
        "    # 5) Early physics hooks present\n",
        "    has_rs = (\"get_rs_star\" in globals() and callable(globals()[\"get_rs_star\"])) or (\"rs_star\" in globals() and callable(globals()[\"rs_star\"]))\n",
        "    _assert(has_rs, \"Missing sound-horizon function: expected callable get_rs_star or rs_star.\")\n",
        "\n",
        "    # Print summary\n",
        "    print(\"====================================================\")\n",
        "    print(\"13.2) Referee checklist — PASSED\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    tag = f\"Policy: {policy}\"\n",
        "    if (policy.upper() == \"LAST_RUN\") and (rid_ref is not None):\n",
        "        tag += f\" | run{int(rid_ref):03d}\"\n",
        "    print(tag)\n",
        "    print(\"FAIR θ5 sizes: OK\")\n",
        "    print(\"Planck lite 3D shapes: OK (R,lA,wb + 3x3 cov/inv_cov)\")\n",
        "    print(\"CMB cov/inv_cov symmetry + COV@INV≈I: OK\")\n",
        "    print(\"chi2 coherence: chi2_tot == sum(parts): OK\")\n",
        "    print(\"Output directories exist: OK\")\n",
        "    print(\"Early-time physics hooks present: OK\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "    print(\"Reference MAP summary (BEST_10.1):\")\n",
        "    print(\"LCDM chi2 =\", float(sumL), \"| parts =\", partsL)\n",
        "    print(\"ECTI chi2 =\", float(sumE), \"| parts =\", partsE)\n",
        "    print(\"Δχ² (ECTI−LCDM) =\", float(sumE - sumL))\n",
        "    print(\"====================================================\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8L3swLp5wW",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# 14) Reproducibility, data availability, and audit checklist\n",
        "\n",
        "This notebook is designed to be **fully auditable** and reproducible under a referee-style standard.\n",
        "\n",
        "---\n",
        "\n",
        "## A) Model definitions (what is and is not modified)\n",
        "\n",
        "### ΛCDM baseline\n",
        "Spatially-flat ΛCDM background with standard radiation content used where required for early-time quantities.\n",
        "\n",
        "### ECTI-P (late-time extension)\n",
        "ECTI-P modifies the background expansion **only** at low redshift via \\(H(z)\\) (see Section 4).\n",
        "- No recombination changes\n",
        "- No Boltzmann solver (CLASS/CAMB)\n",
        "- No TT/TE/EE likelihood, no CLik\n",
        "- No new growth physics parameters\n",
        "\n",
        "Growth for RSD is computed using the **standard GR linear growth equation**, driven by the model background \\(H(z)\\), with no extra degrees of freedom.\n",
        "\n",
        "---\n",
        "\n",
        "## B) Parameterization and fairness (same-k)\n",
        "\n",
        "The sampled parameter vector is identical for both models:\n",
        "\\[\n",
        "\\theta_5 = (H_0,\\ \\Omega_{m0},\\ \\sigma_8,\\ M,\\ \\omega_b).\n",
        "\\]\n",
        "\n",
        "ECTI-P internal parameters \\((\\beta, z_t)\\) are **fixed constants** (not sampled).  \n",
        "Therefore the comparison is strictly **FAIR**: \\(k=5\\) vs \\(k=5\\), implying:\n",
        "\\[\n",
        "\\Delta \\mathrm{AIC} = \\Delta \\mathrm{BIC} = \\Delta \\chi^2.\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## C) Data inputs (exact sources)\n",
        "\n",
        "### Supernovae (Pantheon+SH0ES)\n",
        "- Files: `Pantheon+SH0ES.dat`, `Pantheon+SH0ES_STAT+SYS.cov`\n",
        "- The notebook includes an **auto-download** step from the official Pantheon+SH0ES DataRelease if files are missing.\n",
        "\n",
        "### BAO (DESI 2024)\n",
        "- Loaded from the public likelihood data repository `CobayaSampler/bao_data`\n",
        "- Files used:\n",
        "  - `desi_2024_gaussian_bao_ALL_GCcomb_mean.txt`\n",
        "  - `desi_2024_gaussian_bao_ALL_GCcomb_cov.txt`\n",
        "- BAO predictions are evaluated as \\(D_V/r_s\\) using a **computed** \\(r_s(z_\\*)\\) that depends on \\(\\omega_b\\) (no fixed fiducial \\(r_s\\)).\n",
        "\n",
        "### RSD (\\(f\\sigma_8\\))\n",
        "- Input file: `rsd.csv`\n",
        "- \\(f\\sigma_8(z)\\) is computed from the standard GR linear growth equation using the model background \\(H(z)\\) and the sampled \\(\\sigma_8\\).\n",
        "\n",
        "### KiDS (compressed constraint)\n",
        "- Implemented as a **compressed Gaussian constraint** on an \\(S_8\\)-like combination.\n",
        "- This is **not** the full cosmic shear 2pt likelihood; it is used as an effective summary constraint.\n",
        "\n",
        "### CMB compressed prior (Planck “lite” 3D)\n",
        "- Uses the distance-prior vector \\((R,\\ \\ell_A,\\ \\omega_b)\\) with a **3×3 covariance** (as defined explicitly in Section 5.5).\n",
        "- The mapping uses coherent early-time quantities \\(D_M(z_\\*)\\) and \\(r_s(z_\\*)\\) including standard radiation content and \\(\\omega_b\\).\n",
        "\n",
        "### Planck lensing derived 1D constraint (optional)\n",
        "- Implemented as an independent 1D Gaussian constraint:\n",
        "  \\[\n",
        "  \\sigma_8 \\Omega_m^{0.25} = 0.589 \\pm 0.020,\n",
        "  \\]\n",
        "  from Planck 2018 lensing-only analysis (see Section 5.6).\n",
        "- Toggle: `USE_PLANCK_LENSING` (counts as **one** datum when enabled).\n",
        "\n",
        "---\n",
        "\n",
        "## D) MCMC implementation\n",
        "\n",
        "- Sampler: `emcee` v3\n",
        "- Backend: HDF5 (`emcee.backends.HDFBackend`) for resume-safe runs\n",
        "- Paired execution policy: LCDM_run_i → ECTI_run_i → report \\(\\Delta \\chi^2\\)\n",
        "- MAP extraction is performed post-burn-in.\n",
        "- Diagnostics include:\n",
        "  - total \\(\\chi^2\\)\n",
        "  - \\(\\chi^2\\) per probe\n",
        "  - Gelman–Rubin \\(\\hat{R}\\), integrated autocorrelation time \\(\\tau_{\\mathrm{int}}\\), and ESS\n",
        "\n",
        "---\n",
        "\n",
        "## E) Run-All safety and expected outcome\n",
        "\n",
        "- Execution is controlled by `RUN_MCMC`.\n",
        "- When `RUN_MCMC=False`, the notebook will skip post-MCMC blocks safely.\n",
        "- When `RUN_MCMC=True` and chains exist in `CAMPAIGN_DIR`, running top-to-bottom reproduces:\n",
        "  - the paired ΛCDM and ECTI-P results (same datasets, priors, and toggles),\n",
        "  - the final \\(\\chi^2\\) breakdowns and \\(\\Delta\\chi^2\\),\n",
        "  - the publication figures and LaTeX tables generated by the post-processing sections.\n",
        "\n",
        "---\n",
        "\n",
        "## F) Audit checklist (referee-grade)\n",
        "\n",
        "- [ ] Section 1 scope statements match the implemented pipeline (RSD growth, KiDS compressed, Planck compressed, lensing prior).\n",
        "- [ ] Same-k fairness is explicit: \\(\\theta_5\\) sampled for both models; \\((\\beta,z_t)\\) fixed for ECTI-P.\n",
        "- [ ] When `USE_PLANCK_LENSING=True`, the lensing block is included in:\n",
        "  - total \\(\\chi^2\\),\n",
        "  - \\(\\chi^2\\) breakdown,\n",
        "  - \\(N_{\\mathrm{data}}\\) used for BIC.\n",
        "- [ ] All post-MCMC cells are Run-All safe after kernel restart (no implicit state).\n",
        "- [ ] Repository version has either cleared outputs or avoids leaking machine-local paths."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#End of the notebook"
      ],
      "metadata": {
        "id": "j0QxjOwt69HM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EGj25PjJaWNW",
        "rktu71WIau0w",
        "PlE_rG1qbOul",
        "_7TdYO2JcjkB",
        "yox54pNYkkh4",
        "WbRvjsMZl_39",
        "GmdsagziYieR"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}